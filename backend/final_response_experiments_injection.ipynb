{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5b378c",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac86417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the uploaded files\n",
    "import os\n",
    "from final_response_front_end_main import initialize_system, process_query\n",
    "from user_history_utils import save_chat_pkl_by_embedding\n",
    "from user_history_utils import save_chat_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f115c67",
   "metadata": {},
   "source": [
    "# Intialize Environment (do this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953ab835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "Loading Snowflake/snowflake-arctic-embed-l-v2.0 from local storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake/snowflake-arctic-embed-l-v2.0 embedding model loaded to cuda\n",
      "\n",
      "\n",
      "############### System Prompt:\n",
      "\n",
      "    You are an advanced AI research assistant. Generate detailed and comprehensive responses that supplement students' and academic researchers' work with information grounded in highly cited AI/ML research papers, specifically in fields like NLP and CV. The response should not focus on one area of study but should be informed by both the current query and chat history to generate a well-rounded answer.\n",
      "\n",
      "    1. **Introductory Overview**: Start with a high-level conceptual overview of the topic, providing a brief and clear explanation that covers the essential aspects of the subject. This should be accessible to a broad audience.\n",
      "\n",
      "    2. **Technical Overview**: After the conceptual overview, provide a more in-depth, technical explanation that dives deeper into the topic. This could include relevant algorithms, methods, or models, as well as their theoretical foundations.\n",
      "\n",
      "    3. **Example-Based Expansion**: Throughout the response, incorporate examples from relevant research to illustrate key concepts. These examples should come from generalized research trends and not focus on specific papers or studies, helping to broaden the context.\n",
      "\n",
      "    4. **Broader Exploration**: After addressing the original query, provide suggestions for related topics or areas for further exploration, encouraging the user to expand their understanding. The exploration should relate to the current query and prior query/response pairs, offering natural extensions to the discussion, such as other approaches, applications, or advancements related to the topic.\n",
      "\n",
      "    The tone should be professional yet approachable, offering a balance of conceptual clarity and technical depth. The response should not be overly simplistic, but should aim to make complex topics understandable while offering substantial detail. Use direct quotes where relevant, but focus primarily on summarizing findings from academic research.\n",
      "    \n",
      "Upserted batch of 10443 points to qdrant client collection search_collection\n",
      "Loading chat, start from query number: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "DEVICE, TOKENIZER, EMBEDDING_MODEL, LLM_MODEL, LLM_SYSTEM_PROMPT, QDRANT_CLIENT, CHUNK_COLLECTION, HISTORY_COLLECTION, BM25_SEARCH_FUNCTION, user_query_state_history, query_num, HISTORICAL_QUERY_NUM = initialize_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c393cbd",
   "metadata": {},
   "source": [
    "# Call Response Generation Function (do this as much as you like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f6dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_back_end_response_generation(\n",
    "                                      user_query_state_history: dict,\n",
    "                                      DESIRED_HISTORY_WINDOW_SIZE: int,\n",
    "                                      DESIRED_CONTEXT_CHUNKS_TOP_K: int,\n",
    "                                      RAG_SWITCH: bool,\n",
    "                                      HISTORY_SWITCH: bool,\n",
    "                                      BM25_SWITCH: bool,\n",
    "                                      TOPIC_RETRIEVAL_SWITCH: bool,\n",
    "                                      HISTORIC_QUERY_SIMILARITY_THRESHOLD: float, # [0, 1] range (filter)\n",
    "                                      QUERY_TEXT: str,\n",
    "                                      QUERY_NUM: int,\n",
    "                                      final_json_path: str\n",
    "    ):\n",
    "\n",
    "    # Call the `process_query()` function with the inputs\n",
    "    user_query_state_history[QUERY_NUM] = process_query(\n",
    "        DESIRED_HISTORY_WINDOW_SIZE, \n",
    "        DESIRED_CONTEXT_CHUNKS_TOP_K, \n",
    "        RAG_SWITCH, \n",
    "        HISTORY_SWITCH, \n",
    "        BM25_SWITCH, \n",
    "        TOPIC_RETRIEVAL_SWITCH, \n",
    "        HISTORIC_QUERY_SIMILARITY_THRESHOLD, \n",
    "        QUERY_TEXT, \n",
    "        user_query_state_history,\n",
    "        QUERY_NUM, \n",
    "        QDRANT_CLIENT, \n",
    "        CHUNK_COLLECTION,\n",
    "        HISTORY_COLLECTION,\n",
    "        LLM_MODEL,\n",
    "        LLM_SYSTEM_PROMPT,\n",
    "        DEVICE,\n",
    "        EMBEDDING_MODEL, \n",
    "        TOKENIZER,\n",
    "        BM25_SEARCH_FUNCTION\n",
    "    )\n",
    "\n",
    "    return user_query_state_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d02dae",
   "metadata": {},
   "source": [
    "# Input Test Query Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c543e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list_of_queries_for_testing(\n",
    "                            saved_chats_topic_name, # The name of the folder to save questions in\n",
    "                            list_of_questions, # The list of questions being used to get responses\n",
    "                            task_folder='experiments',\n",
    "                            user_output_folder='my_experiment', # the folder to save the chats in\n",
    "                            DESIRED_HISTORY_WINDOW_SIZE=0, # The lookback window size for history utilization\n",
    "                            DESIRED_CONTEXT_CHUNKS_TOP_K=5, # The number of chunks requested for a question\n",
    "                            RAG_SWITCH=True, # Front-end user input for enabling RAG\n",
    "                            HISTORY_SWITCH=False, # Front-end user input for enabling history usage\n",
    "                            BM25_SWITCH=False, # Front-end user input for enabling BM25 search\n",
    "                            TOPIC_RETRIEVAL_SWITCH=False, # Front-end user input for enabling topic retrieval\n",
    "                            HISTORIC_QUERY_SIMILARITY_THRESHOLD=0.3 # History query-to-query similarity filter min bound\n",
    "    ):\n",
    "\n",
    "    directory = f'user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Define the name of the chat, and the .json / .pkl files that will be saved with it\n",
    "    chat_json_name = f\"{saved_chats_topic_name}.json\"\n",
    "    chat_embedded_history_name = 'user_embedded_history.pkl'\n",
    "    chat_non_embedded_history_name = 'user_non_embedded_history.pkl'\n",
    "\n",
    "    # Define the path of the json / pkl files\n",
    "    final_json_path = os.path.join(directory, chat_json_name)\n",
    "    final_embedded_history_path = os.path.join(directory, chat_embedded_history_name)\n",
    "    final_non_embedded_history_path = os.path.join(directory, chat_non_embedded_history_name)\n",
    "\n",
    "    # Define the initial user state\n",
    "    user_state = {}\n",
    "\n",
    "    for i, question in enumerate(list_of_questions):\n",
    "        query_num = i + 1\n",
    "        # Your processing logic goes here (e.g., semantic search, BM25, RAG)\n",
    "        print(f\"Processing query {query_num}: {question}\\n\")\n",
    "        print(f\"RAG Switch: {RAG_SWITCH}\")\n",
    "        print(f\"History Switch: {HISTORY_SWITCH}\")\n",
    "        print(f\"BM25 Switch: {BM25_SWITCH}\")\n",
    "        print(f\"Topic Retrieval Switch: {TOPIC_RETRIEVAL_SWITCH}\\n\")\n",
    "\n",
    "        user_state = call_back_end_response_generation(\n",
    "                                          user_state,\n",
    "                                          DESIRED_HISTORY_WINDOW_SIZE,\n",
    "                                          DESIRED_CONTEXT_CHUNKS_TOP_K,\n",
    "                                          RAG_SWITCH,\n",
    "                                          HISTORY_SWITCH,\n",
    "                                          BM25_SWITCH,\n",
    "                                          TOPIC_RETRIEVAL_SWITCH,\n",
    "                                          HISTORIC_QUERY_SIMILARITY_THRESHOLD,\n",
    "                                          QUERY_TEXT=question,\n",
    "                                          final_json_path=final_json_path,\n",
    "                                          QUERY_NUM=query_num\n",
    "        )\n",
    "\n",
    "        save_chat_json(user_state[query_num], file_path=final_json_path)\n",
    "\n",
    "        #print(f\"Response: {user_state[query_num]['response_text']}\\n\")\n",
    "\n",
    "    # After processing the query, you might want to save the results to disk or perform further actions\n",
    "    save_chat_pkl_by_embedding(user_state, \n",
    "                                embedded_path=final_embedded_history_path,\n",
    "                                non_embedded_path=final_non_embedded_history_path\n",
    "    )\n",
    "\n",
    "    print(f'#### Final User State Length: {len(user_state)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee04f2d",
   "metadata": {},
   "source": [
    "# Experiment Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0223e7",
   "metadata": {},
   "source": [
    "## My Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28768dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_input_questions = [\n",
    "#'Please explain batch normalization.',                                    # 1\n",
    "#'How does it relate to layer normalization?',                             # 2\n",
    "#'Can you explain the advantages of each method?']                         # 3\n",
    "# 'What are some real-world applications of batch normalization?',          # 4\n",
    "# 'How does batch normalization affect the performance of neural networks?',# 5\n",
    "# 'What are the key differences between normalization techniques?',         # 6\n",
    "# #Hard topic shift begins here:\n",
    "# 'What is reinforcement learning and how does it work?',                   # 7\n",
    "# 'Can you explain the exploration-exploitation tradeoff?',                 # 8\n",
    "# 'What are some popular algorithms used in reinforcement learning?',       # 9\n",
    "# 'asdfadsgfasd',                                                           # 10\n",
    "# 'exit'                                                                    # end\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015fac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input_questions_HW2 = [\n",
    "\"What is the purpose of tokenizing text into sentences and words?\",\n",
    "\n",
    "\"What are n-gram language models and how are they useful in NLP?\",\n",
    "\n",
    "\"What is the naive Bayes assumption and how does it relate to text classificatIon?\",\n",
    "\n",
    "\"What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\",\n",
    "\n",
    "\"What do we mean by \\\"features\\\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\",\n",
    "\n",
    "\"What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\",\n",
    "\n",
    "\"What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\",\n",
    "\n",
    "\"How could you determine which features are most important or indicative for a logistic regression text classification model?\",\n",
    "\n",
    "\"What is the bag-of-words representation and what are some of its limitations for text classification?\",\n",
    "\n",
    "\"What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f1201e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input_questions_HW4 = [\n",
    "\"What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\",\n",
    "\n",
    "\"Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\",\n",
    "\n",
    "\"What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\",\n",
    "\n",
    "\"Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\",\n",
    "\n",
    "\"Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\",\n",
    "\n",
    "\"Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\",\n",
    "\n",
    "\"Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9aa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_input_questions_HW5 = [\n",
    "    \"Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\",\n",
    "\n",
    "    \"What is multi-head attention and why is it useful? How is it implemented in the Transformer?\",\n",
    "\n",
    "    \"Explain how positional encodings work in the Transformer and why they are necessary.\",\n",
    "\n",
    "    \"What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\",\n",
    "\n",
    "    \"Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\",\n",
    "\n",
    "    \"How do large language models like GPT-3 differ from the original Transformer model described in the paper?\",\n",
    "\n",
    "    \"Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\",\n",
    "\n",
    "    \"What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\",\n",
    "\n",
    "    \"Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\",\n",
    "    \n",
    "    \"Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a86a2",
   "metadata": {},
   "source": [
    "## Experiment Subset @top_k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75abb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_top_k_chunks_3 = 3 # @ the number of chunks in the system\n",
    "\n",
    "zero_shot_experiments_at_3_top_k = {\n",
    "\n",
    "    # experiment_1: Naive response without retrieval\n",
    "    'experiment_1' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': False, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_2: RAG response with basic retrieval\n",
    "    'experiment_2' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_3: RAG response hybrid retrieval\n",
    "    'experiment_3' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_4: RAG response hybrid with topic level retrieval\n",
    "    'experiment_4' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': True, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "history_window_size = 5\n",
    "\n",
    "multi_shot_experiments_at_3_top_k = {\n",
    "\n",
    "    # experiment_5: Naive response without retrieval and history\n",
    "    'experiment_5' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': False, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_6: RAG response with basic retrieval and history\n",
    "    'experiment_6' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_7: RAG response hybrid retrieval and history\n",
    "    'experiment_7' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_8: RAG response hybrid with topic level retrieval and history\n",
    "    'experiment_8' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_3, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': True, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7be763",
   "metadata": {},
   "source": [
    "## Experiment Subset @top_k=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8503fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_top_k_chunks_5 = 5 # @ the number of chunks in the system\n",
    "\n",
    "zero_shot_experiments_at_5_top_k = {\n",
    "\n",
    "    # experiment_9: Naive response without retrieval\n",
    "    'experiment_9' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': False, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_10: RAG response with basic retrieval\n",
    "    'experiment_10' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_11: RAG response hybrid retrieval\n",
    "    'experiment_11' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_12: RAG response hybrid with topic level retrieval\n",
    "    'experiment_12' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': 0, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': False, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': True, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "history_window_size = 5\n",
    "\n",
    "multi_shot_experiments_at_5_top_k = {\n",
    "\n",
    "    # experiment_13: Naive response without retrieval and history\n",
    "    'experiment_13' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': False, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_14: RAG response with basic retrieval and history\n",
    "    'experiment_14' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': False, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_15: RAG response hybrid retrieval and history\n",
    "    'experiment_15' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': False, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    },\n",
    "\n",
    "    # experiment_16: RAG response hybrid with topic level retrieval and history\n",
    "    'experiment_16' : {\n",
    "        'DESIRED_HISTORY_WINDOW_SIZE': history_window_size, # The lookback window size for history utilization\n",
    "        'DESIRED_CONTEXT_CHUNKS_TOP_K': at_top_k_chunks_5, # The number of chunks requested for a question\n",
    "        'RAG_SWITCH': True, # Front-end user input for enabling RAG\n",
    "        'HISTORY_SWITCH': True, # Front-end user input for enabling history usage\n",
    "        'BM25_SWITCH': True, # Front-end user input for enabling BM25 search\n",
    "        'TOPIC_RETRIEVAL_SWITCH': True, # Front-end user input for enabling topic retrieval\n",
    "        'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b110b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "\n",
    "def conduct_experiment(my_input_questions: list, experiment: dict, experiment_type: str):\n",
    "\n",
    "    # Process each experiment in the selected experiment type\n",
    "    for experiment_name, experiment_config in experiment.items():\n",
    "        print(f\"#### Running {experiment_name} with configuration: {experiment_config}\\n\")\n",
    "        \n",
    "        # Here you can customize the configuration further if needed (e.g., dynamic changes)\n",
    "        process_list_of_queries_for_testing(\n",
    "            saved_chats_topic_name=experiment_name, \n",
    "            list_of_questions=my_input_questions,\n",
    "            task_folder='experiments',\n",
    "            user_output_folder=experiment_type,\n",
    "            DESIRED_HISTORY_WINDOW_SIZE=experiment_config['DESIRED_HISTORY_WINDOW_SIZE'],\n",
    "            DESIRED_CONTEXT_CHUNKS_TOP_K=experiment_config['DESIRED_CONTEXT_CHUNKS_TOP_K'],\n",
    "            RAG_SWITCH=experiment_config['RAG_SWITCH'],\n",
    "            HISTORY_SWITCH=experiment_config['HISTORY_SWITCH'],\n",
    "            BM25_SWITCH=experiment_config['BM25_SWITCH'],\n",
    "            TOPIC_RETRIEVAL_SWITCH=experiment_config['TOPIC_RETRIEVAL_SWITCH'],\n",
    "            HISTORIC_QUERY_SIMILARITY_THRESHOLD=experiment_config['HISTORIC_QUERY_SIMILARITY_THRESHOLD']\n",
    "        )\n",
    "\n",
    "        #time.sleep(61)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce02e8",
   "metadata": {},
   "source": [
    "## HW 2 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcfcd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_1 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_2 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_3 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_4 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW2,\n",
    "                   experiment=zero_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='zero_shot_HW2_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84169b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_9 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_10 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_11 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_12 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW2,\n",
    "                   experiment=zero_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='zero_shot_HW2_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ebbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW2,\n",
    "                   experiment=multi_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='multi_shot_HW2_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78680970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What is the purpose of tokenizing text into sentences and words?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What are n-gram language models and how are they useful in NLP?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What is the naive Bayes assumption and how does it relate to text classificatIon?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What are some of the advantages and disadvantages of naive Bayes classifiers compared to logistic regression?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: What do we mean by \"features\" in the context of text classification? Give some examples of features that might be useful for distinguishing different newsgroup topics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: What is the purpose of a test set in machine learning? Why do we need separate training and test sets?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: What metrics could you use to evaluate the performance of a text classification model? Define accuracy and any other relevant metrics.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: How could you determine which features are most important or indicative for a logistic regression text classification model?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: What is the bag-of-words representation and what are some of its limitations for text classification?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: What is overfitting in machine learning? How could you tell if your text classification model is overfitting the training data? Describe two ways to reduce overfitting.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW2,\n",
    "                   experiment=multi_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='multi_shot_HW2_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c624ed4",
   "metadata": {},
   "source": [
    "## HW 4 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900b69d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_1 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_2 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_3 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_4 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW4,\n",
    "                   experiment=zero_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='zero_shot_HW4_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e72b1d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_9 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_10 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_11 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_12 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW4,\n",
    "                   experiment=zero_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='zero_shot_HW4_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "133e44e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW4,\n",
    "                   experiment=multi_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='multi_shot_HW4_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8b01891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: What are the key components of the encoder-decoder architecture for sequence-to-sequence models? How is this architecture used for machine translation?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: Attention mechanisms have become an integral part of sequence-to-sequence models. Explain how attention works and why it improves performance compared to basic encoder-decoder models.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: What techniques can be used to handle very long input or output sequences in seq2seq models? Discuss solutions like hierarchical attention, sparse attention, and other approaches.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: Explain how LSTMs and other RNN architectures are commonly used for sequence labeling tasks like part-of-speech tagging. What advantages do they provide over Hidden Markov Model (HMM) based techniques?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Compare part-of-speech (POS) tagging and named entity recognition (NER) tasks. What are the key differences in terms of input representation, output labels, model architecture, and evaluation metrics? Discuss challenges unique to NER such as entity boundary detection.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: Explain the core components of the LSTM unit - the cell state, input gate, forget gate, output gate. How does this gating mechanism address shortcomings of basic RNNs?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Despite gating mechanisms, LSTMs can still face challenges in learning long-term dependencies. Explain limitations of standard LSTMs for modeling long sequences. Discuss at least two techniques that can help LSTMs better capture long-range dependencies, such as dilated LSTMs, skipping connections, and attention.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW4,\n",
    "                   experiment=multi_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='multi_shot_HW4_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f3de0",
   "metadata": {},
   "source": [
    "## HW 5 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52301c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_1 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_2 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_3 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_4 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW5,\n",
    "                   experiment=zero_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='zero_shot_HW5_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0e8eaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_9 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_10 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_11 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_12 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 0, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': False, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: False\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zero Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW5,\n",
    "                   experiment=zero_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='zero_shot_HW5_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a225ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 3, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=3\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW5,\n",
    "                   experiment=multi_shot_experiments_at_3_top_k,\n",
    "                   experiment_type='multi_shot_HW5_at_3_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0573fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Running experiment_5 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': False, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: False\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_6 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': False, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: False\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_7 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': False, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: False\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n",
      "#### Running experiment_8 with configuration: {'DESIRED_HISTORY_WINDOW_SIZE': 5, 'DESIRED_CONTEXT_CHUNKS_TOP_K': 5, 'RAG_SWITCH': True, 'HISTORY_SWITCH': True, 'BM25_SWITCH': True, 'TOPIC_RETRIEVAL_SWITCH': True, 'HISTORIC_QUERY_SIMILARITY_THRESHOLD': 0.3}\n",
      "\n",
      "Processing query 1: Explain the overall architecture of the Transformer model. What are the main components of the encoder and decoder?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 2: What is multi-head attention and why is it useful? How is it implemented in the Transformer?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 3: Explain how positional encodings work in the Transformer and why they are necessary.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 4: What is the purpose of layer normalization and residual connections in the Transformer? Where are they applied?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 5: Describe the training process for the Transformer. What is the batching scheme used? What is label smoothing and why is it helpful?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 6: How do large language models like GPT-3 differ from the original Transformer model described in the paper?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 7: Explain the pre-training and fine-tuning process for large language models. Why is pre-training on large unlabeled corpora important?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 8: What are some of the key challenges in training very large language models? Discuss techniques like sparse attention and model parallelism.\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 9: Large language models have shown impressive few-shot learning abilities. What factors contribute to this? How could we further improve few-shot learning?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "Processing query 10: Discuss the risks and ethical considerations with large language models. What should we be cautious about when deploying them in real applications? How can we make them safer and more trustworthy?\n",
      "\n",
      "RAG Switch: True\n",
      "History Switch: True\n",
      "BM25 Switch: True\n",
      "Topic Retrieval Switch: True\n",
      "\n",
      "✅ Attempt 1: STOP with content (code 1).\n",
      "\n",
      "#### Final User State Length: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot @top_k=5\n",
    "conduct_experiment(my_input_questions=my_input_questions_HW5,\n",
    "                   experiment=multi_shot_experiments_at_5_top_k,\n",
    "                   experiment_type='multi_shot_HW5_at_5_top_k'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f9260",
   "metadata": {},
   "source": [
    "# Small Unit Test\n",
    "\n",
    "Function from: `model_prompting_utils.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44baa550",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_chunk_ids_by_bottom_level_headers(chunk_ids_by_header, example_headers):\n",
    "    \"\"\"\n",
    "    Get a dictionary of chunk IDs that match the bottom-level sections of the original header tuples.\n",
    "    This function handles malformed or empty headers by assigning unique placeholders\n",
    "    to preserve distinction and avoid key collisions in the dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Step 1: Safely extract bottom-level headers ===\n",
    "    bottom_level_headers = []\n",
    "    header_placeholder_counter = 0\n",
    "\n",
    "    for header in example_headers:\n",
    "        if len(header) > 0:\n",
    "            bottom_level_headers.append(header[-1])\n",
    "        else:\n",
    "            placeholder = f\"ERROR_NO_HEADER_PLACEHOLDER_{header_placeholder_counter}\"\n",
    "            print(f\"⚠ Assigned unique placeholder to bottom_level_headers: {placeholder}\")\n",
    "            bottom_level_headers.append(placeholder)\n",
    "            header_placeholder_counter += 1\n",
    "\n",
    "    if header_placeholder_counter > 1:\n",
    "        print(f\"!!! MULTIPLE_UNIQUE_PLACEHOLDERS_IN_HEADERS: {header_placeholder_counter} unknown example_headers assigned unique placeholders\")\n",
    "\n",
    "    # === Step 2: Normalize keys in chunk_ids_by_header ===\n",
    "    normalized_chunk_ids_by_header = {}\n",
    "    chunk_placeholder_counter = 0\n",
    "\n",
    "    for key, value in chunk_ids_by_header.items():\n",
    "        if key is None or (isinstance(key, str) and key.strip() == ''):\n",
    "            placeholder_key = f\"ERROR_NO_HEADER_PLACEHOLDER_{chunk_placeholder_counter}\"\n",
    "            print(f\"⚠ Assigned unique placeholder key to chunk_ids_by_header: {placeholder_key} for value: {value}\")\n",
    "            normalized_chunk_ids_by_header[placeholder_key] = value\n",
    "            chunk_placeholder_counter += 1\n",
    "        else:\n",
    "            normalized_chunk_ids_by_header[key] = value\n",
    "\n",
    "    if chunk_placeholder_counter > 1:\n",
    "        print(f\"!!! MULTIPLE_UNIQUE_PLACEHOLDERS_IN_CHUNKS: {chunk_placeholder_counter} malformed chunk_ids_by_header keys assigned unique placeholders\")\n",
    "\n",
    "    # === Step 3: Filter normalized chunk IDs by bottom-level headers ===\n",
    "    filtered_chunk_ids_by_header = {}\n",
    "    print(\"\\nFiltering normalized_chunk_ids_by_header based on bottom_level_headers...\")\n",
    "    print(\"Bottom-level headers:\", bottom_level_headers, \"\\n\")\n",
    "\n",
    "    for key, value in normalized_chunk_ids_by_header.items():\n",
    "        if key in bottom_level_headers:\n",
    "            print(f\"✔ Match found: {repr(key)} is in bottom_level_headers. Adding to result.\")\n",
    "            filtered_chunk_ids_by_header[key] = value\n",
    "        else:\n",
    "            print(f\"✘ No match: {repr(key)} not in bottom_level_headers. Skipping.\")\n",
    "\n",
    "    print(\"\\nFiltered result keys:\", list(filtered_chunk_ids_by_header.keys()))\n",
    "\n",
    "    return filtered_chunk_ids_by_header\n",
    "\n",
    "chunk_ids_by_header = {'**SegFormer: Simple and Efficient Design for Semantic** **Segmentation with Transformers**': [9405, 9406, 9407, 9408, 9409], '**Abstract**': [9405], '**1 Introduction**': [9406, 9407, 9408], '**2 Related Work**': [9409], '': [9410, 9411, 9412, 9413, 9414, 9415, 9416, 9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424, 9425, 9426, 9427, 9428, 9429, 9430, 9431, 9432], '**3 Method**': [9412, 9413, 9414, 9415, 9416], '**4 Experiments**': [9417, 9418, 9419, 9420, 9421, 9422, 9423, 9424], '**5 Conclusion**': [9425], '**Broader Impact**': [9426], '**A Details of MiT Series**': [9427], '**B More Qualitative Results on Mask Predictions**': [9428], '**C More Visualization on Effective Receptive Field**': [9429], '**D More Comparison of DeeplabV3+ and SegFormer on Cityscapes-C**': [9430, 9431, 9432]} \n",
    "\n",
    "example_headers = [('**4 Experiments**',), (), (), ('**3 Method**',), ('**SegFormer: Simple and Efficient Design for Semantic** **Segmentation with Transformers**', '**Abstract**'), ('**SegFormer: Simple and Efficient Design for Semantic** **Segmentation with Transformers**', '**1 Introduction**')] \n",
    "\n",
    "\n",
    "get_chunk_ids_by_bottom_level_headers(chunk_ids_by_header=chunk_ids_by_header, \n",
    "                                      example_headers=example_headers\n",
    "                                      )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2dbdcc",
   "metadata": {},
   "source": [
    "# Single Query Example: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805ad24",
   "metadata": {},
   "source": [
    "```python\n",
    "# Collect user input and options dynamically from the front-end\n",
    "# For demonstration, we're simulating user input here. In your front-end, these values will be collected from user interactions.\n",
    "\n",
    "# Example user inputs (to be passed from the front-end)\n",
    "DESIRED_HISTORY_WINDOW_SIZE = 3\n",
    "DESIRED_CONTEXT_CHUNKS_TOP_K = 5\n",
    "\n",
    "RAG_SWITCH = True  # Front-end user input for enabling RAG\n",
    "HISTORY_SWITCH = False  # Front-end user input for enabling history usage\n",
    "BM25_SWITCH = True  # Front-end user input for enabling BM25 search\n",
    "TOPIC_RETRIEVAL_SWITCH = False  # Front-end user input for enabling topic retrieval\n",
    "\n",
    "HISTORIC_QUERY_SIMILARITY_THRESHOLD = 0.3  # Front-end user input for similarity threshold\n",
    "\n",
    "QUERY_TEXT = 'What is a neural radience field NERF?'\n",
    "#QUERY_TEXT = input(\"Enter your question (or type 'exit' to quit): \")  # Get the query text from user input\n",
    "\n",
    "# Your processing logic goes here (e.g., semantic search, BM25, RAG)\n",
    "print(f\"Processing query: {QUERY_TEXT}\")\n",
    "print(f\"RAG Switch: {RAG_SWITCH}\")\n",
    "print(f\"History Switch: {HISTORY_SWITCH}\")\n",
    "print(f\"BM25 Switch: {BM25_SWITCH}\")\n",
    "print(f\"Topic Retrieval Switch: {TOPIC_RETRIEVAL_SWITCH}\")\n",
    "\n",
    "# This incriment signals the initialization of a new query state placement matters\n",
    "query_num += 1\n",
    "\n",
    "# Call the `process_query()` function with the inputs\n",
    "user_query_state_history[query_num] = process_query(\n",
    "    DESIRED_HISTORY_WINDOW_SIZE, \n",
    "    DESIRED_CONTEXT_CHUNKS_TOP_K, \n",
    "    RAG_SWITCH, \n",
    "    HISTORY_SWITCH, \n",
    "    BM25_SWITCH, \n",
    "    TOPIC_RETRIEVAL_SWITCH, \n",
    "    HISTORIC_QUERY_SIMILARITY_THRESHOLD, \n",
    "    QUERY_TEXT, \n",
    "    user_query_state_history,\n",
    "    query_num, \n",
    "    QDRANT_CLIENT, \n",
    "    CHUNK_COLLECTION,\n",
    "    HISTORY_COLLECTION,\n",
    "    LLM_MODEL,\n",
    "    LLM_SYSTEM_PROMPT,\n",
    "    DEVICE,\n",
    "    EMBEDDING_MODEL, \n",
    "    TOKENIZER,\n",
    "    BM25_SEARCH_FUNCTION\n",
    ")\n",
    "\n",
    "save_chat_json(user_query_state_history[query_num])\n",
    "\n",
    "print(f\"Processed query {query_num}: {QUERY_TEXT}\")\n",
    "print(f\"Response: {user_query_state_history[query_num]['response_text']}\")\n",
    "\n",
    "# After processing the query, you might want to save the results to disk or perform further actions\n",
    "save_chat_pkl_by_embedding(user_query_state_history, \n",
    "                            embedded_path='user_output/user_embedded_history.pkl',\n",
    "                            non_embedded_path='user_output/user_non_embedded_history.pkl'\n",
    ")\n",
    "\n",
    "# Optionally, you could display or log the results of the query here for debugging purposes:\n",
    "print(f\"Processed query {query_num}: {QUERY_TEXT}\")\n",
    "print(f\"Response: {user_query_state_history[query_num]['response_text']}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
