# -*- coding: utf-8 -*-
"""NLP FINAL PROJECT - Qdrant Snowflake Embedding Vector Search + Response Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GpvLRwQgAVJeSItGM2x95QE4HXBV4Bhd
"""

# Data Loading
import pickle

# Embedding Model
from transformers import AutoModel, AutoTokenizer
import torch

# Qdrant Vector Database
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from qdrant_client.models import Batch

# Gemini LLM Output
import google.generativeai as genai

"""# Embedding modelsnowflake-arctic-embed-l-v2.0:

https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0

https://huggingface.co/learn/cookbook/en/code_search
"""

# Load model and tokenizer
model_name = 'Snowflake/snowflake-arctic-embed-l-v2.0'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name, add_pooling_layer=False)
model.eval()

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device = "cpu"
model.to(device)  # Ensure model runs on GPU

# Get hidden size (embedding size)
embedding_size = model.config.hidden_size
print(f"Embedding size: {embedding_size}")

"""# Initializing Qdrant Database (In Memory)

https://qdrant.tech/documentation/quickstart/

https://python-client.qdrant.tech/qdrant_client.qdrant_client

https://colab.research.google.com/github/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb#scrollTo=-nlMbYPkJnSS
"""

qdrant_client = QdrantClient(location=":memory:")

MY_COLLECTION = "search_collection"

first_collection = qdrant_client.recreate_collection(
    collection_name=MY_COLLECTION,
    vectors_config=VectorParams(size=embedding_size, # Size of Snowflake Embedding Dimensions
                                distance=Distance.COSINE), # Cosine similarity for vector search
)

collection_info = qdrant_client.get_collection(collection_name=MY_COLLECTION)
list(collection_info)

"""# Loading Data from JSON collection"""

# Load the .pkl file
with open("qdrant_vectors.pkl", "rb") as f:
    data = pickle.load(f)  # This should be a list of dicts with keys: "id", "vector", "payload"

# Extract ids, vectors, and payloads from Record objects
ids = [record.id for record in data]
vectors = [record.vector for record in data]
payloads = [record.payload for record in data]

"""https://qdrant.tech/documentation/concepts/points/"""


# Create a Batch object
my_batch = Batch(ids=ids, vectors=vectors, payloads=payloads)

first_collection = qdrant_client.upsert(
    collection_name=MY_COLLECTION,
    points=my_batch
)

"""# Applying a Similarity Search Via Query Input"""

def get_query_embedding(query_text):
    query_tokens = tokenizer(query_text, padding=True, truncation=True, return_tensors='pt', max_length=8192)

    with torch.no_grad():
        query_embedding = model(**query_tokens)[0][:, 0]

    norm_query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)

    return norm_query_embedding.numpy().tolist()[0]

# Gets closest similarity score chunks
def get_context_chunks(norm_query_embedding, num_chunks=5):

    context_chunks = qdrant_client.query_points(
                      collection_name=MY_COLLECTION,
                      query=norm_query_embedding,
                      limit=num_chunks
    ).points

    #print(context_chunks)

    return context_chunks

def process_provided_chunks(input_context_chunks):

    # Process each context chunk and save relevant information
    provided_context = []
    for idx, chunk in enumerate(input_context_chunks):
        save_query_context = {}
        
        # Save relevant information from each chunk into the dictionary
        save_query_context['qdrant_id'] = chunk.id
        save_query_context['score'] = chunk.score
        save_query_context['text'] = chunk.payload['text']
        save_query_context['arxiv_id'] = chunk.payload['pdf_metadata']['id']
        save_query_context['title'] = chunk.payload['pdf_metadata']['title']
        
        # Append the dictionary to the provided_context list
        provided_context.append(save_query_context)
    
    return provided_context

def get_texts_by_ids(ids, collection_name):
    if ids:
        results = qdrant_client.retrieve(
            collection_name=collection_name,
            ids=ids,
            with_payload=True
        )

        # Build a lookup dictionary for fast access
        id_to_record = {point.id: point.payload for point in results}

        # Reconstruct ordered dict based on input ID order
        records_by_id = [{**id_to_record.get(id_, {"text": "", "pdf_metadata": None})} for id_ in ids]

        return records_by_id
    else:
        print('No Context Provided')
        return []

"""# Generate response with Gemini with vs without context."""

# Needs to be adjusted to take a varible amount of chunks
def prompt_model(input_query, context=False, k=5):
    context_chunks = []
    context_chunk_ids = []

    if context:
        # Get Query Embedding and retrieve relevant Context
        norm_query_embeddings = get_query_embedding(input_query)
        context_chunks = get_context_chunks(norm_query_embeddings, num_chunks=k)
        print(f'Number of Potential Context Chunks: {len(context_chunks)}')

        top_k = min(k, len(context_chunks))
        print(f'Chosen Chunks {top_k}')

        full_prompt = ''
        # Loop through each context chunk and add its text to the prompt
        for chunk in context_chunks:
            full_prompt += f"{chunk.payload['text']}\n"
            context_chunk_ids.append(chunk.id)  # Collect the ID

        # Add the question part
        full_prompt += f"\nQuestion:\n{input_query}\n"

        # Final prompt with instructions (a.k.a system prompt)
        system_prompt = "\nPlease provide a comprehensive response to the question using the above context. Draw from the context as well as innate knowledge to synthesize a coherent result."

        full_prompt += system_prompt
        response = LLM_MODEL.generate_content(full_prompt)

    else:
        full_prompt = f"""
        Question:
        {input_query}

        Please provide a well-reasoned response to the question.
        """
        response = LLM_MODEL.generate_content(full_prompt)

    return response, context_chunks, context_chunk_ids

def generate_markdown_from_history(history):
    file_no_context = 'response_no_context.md'
    file_with_context = 'response_with_context.md'
    context_file = 'context_details.md'

    with open(file_no_context, 'w', encoding='utf-8') as f1:
        for query, response_no_context in zip(history["queries"], history["responses_no_context"]):
            f1.write(f"## Query\n\n{query}\n\n")
            f1.write(f"## Response (No Context)\n\n{response_no_context}\n\n")

    with open(file_with_context, 'w', encoding='utf-8') as f2:
        for query, response_with_context in zip(history["queries"], history["responses_with_context"]):
            f2.write(f"## Query\n\n{query}\n\n")
            f2.write(f"## Response (With Context)\n\n{response_with_context}\n\n")
    
    with open("context_details.md", "w", encoding='utf-8') as f3:
        for query_idx, (query, context_details) in enumerate(zip(user_history["queries"], user_history["provided_context"])):
            if query_idx == 0:
                f3.write(f"## Query {query_idx + 1}\n\n{query}\n\n")
            else:
                f3.write(f"\n\n## Query {query_idx + 1}\n\n{query}\n\n")

            for idx, chunk in enumerate(context_details):
                f3.write(f"### Context Chunk {idx + 1}############\n")
                f3.write(f"**ID:** {chunk['qdrant_id']}\n")
                f3.write(f"**Score:** {chunk['score']:.4f}\n")
                f3.write(f"**ArXiv ID:** {chunk['arxiv_id']}\n")
                f3.write(f"**Title:** {chunk['title']}\n")
                f3.write(f"**Text:** {chunk['text']}\n")

    print(f"Markdown files updated: {file_no_context}, {file_with_context}, {context_file}")

""" link to get api key - https://aistudio.google.com/apikey"""

GEMINI_API_KEY = 'AIzaSyAMoV9tEHL-2w12gMnzM98d5EV9jhzPw-E'

# Set up API key
genai.configure(api_key=GEMINI_API_KEY)

# Define the model name
MODEL_NAME = "gemini-2.0-flash-lite"

# Initialize the Gemini model
LLM_MODEL = genai.GenerativeModel(MODEL_NAME)

# Initialize user history with separate lists for queries, responses without context, responses with context, and the context itself
user_history = {
    "queries": [],
    "responses_no_context": [],
    "responses_with_context": [],
    "provided_context" : []
}

def main():
    while True:
        # Get user query
        QUERY_TEXT = input("\nEnter your question (or type 'exit' to quit): ")

        if QUERY_TEXT.lower() == 'exit':
            break

        # Add current query to user history
        user_history["queries"].append(QUERY_TEXT)

        # Get response without context (BASE LLM)
        response_no_context, _, _ = prompt_model(input_query=QUERY_TEXT, context=False)

        # Get response with context (RAG LLM)
        response_with_context, context_chunks, context_ids = prompt_model(input_query=QUERY_TEXT, context=True)

        # Add the responses to user history
        user_history["responses_no_context"].append(response_no_context.text)
        user_history["responses_with_context"].append(response_with_context.text)

        provided_context = process_provided_chunks(context_chunks)

        # Add processed context to user history
        user_history["provided_context"].append(provided_context)

        # Generate and save Markdown content
        generate_markdown_from_history(user_history)

if __name__ == "__main__":
    main()
