{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5b378c",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac86417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the uploaded files\n",
    "import os\n",
    "from final_response_front_end_main import initialize_system, process_query\n",
    "from final_response_front_end_main import load_query_history\n",
    "from user_history_utils import save_chat_pkl_by_embedding\n",
    "from user_history_utils import save_chat_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a83266",
   "metadata": {},
   "source": [
    "# Setups:\n",
    "\n",
    "### Below are the various setups of environment variables and functions that are required for **EVERY SESSION**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f115c67",
   "metadata": {},
   "source": [
    "## Intialize System Environment (do this once PER BOOT OF SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953ab835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "Loading Snowflake/snowflake-arctic-embed-l-v2.0 from local storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake/snowflake-arctic-embed-l-v2.0 embedding model loaded to cuda\n",
      "\n",
      "\n",
      "############### System Prompt:\n",
      "\n",
      "    You are an advanced AI research assistant. Generate detailed and comprehensive responses that supplement students' and academic researchers' work with information grounded in highly cited AI/ML research papers, specifically in fields like NLP and CV. The response should not focus on one area of study but should be informed by both the current query and chat history to generate a well-rounded answer.\n",
      "\n",
      "    1. **Introductory Overview**: Start with a high-level conceptual overview of the topic, providing a brief and clear explanation that covers the essential aspects of the subject. This should be accessible to a broad audience.\n",
      "\n",
      "    2. **Technical Overview**: After the conceptual overview, provide a more in-depth, technical explanation that dives deeper into the topic. This could include relevant algorithms, methods, or models, as well as their theoretical foundations.\n",
      "\n",
      "    3. **Example-Based Expansion**: Throughout the response, incorporate examples from relevant research to illustrate key concepts. These examples should come from generalized research trends and not focus on specific papers or studies, helping to broaden the context.\n",
      "\n",
      "    4. **Broader Exploration**: After addressing the original query, provide suggestions for related topics or areas for further exploration, encouraging the user to expand their understanding. The exploration should relate to the current query and prior query/response pairs, offering natural extensions to the discussion, such as other approaches, applications, or advancements related to the topic.\n",
      "\n",
      "    The tone should be professional yet approachable, offering a balance of conceptual clarity and technical depth. The response should not be overly simplistic, but should aim to make complex topics understandable while offering substantial detail. Use direct quotes where relevant, but focus primarily on summarizing findings from academic research.\n",
      "    \n",
      "Upserted batch of 10443 points to qdrant client collection search_collection\n",
      "Loading chat, start from query number: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "DEVICE, TOKENIZER, EMBEDDING_MODEL, LLM_MODEL, LLM_SYSTEM_PROMPT, QDRANT_CLIENT, CHUNK_COLLECTION, HISTORY_COLLECTION, BM25_SEARCH_FUNCTION, _, _, _ = initialize_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d02dae",
   "metadata": {},
   "source": [
    "## Initialize a user's chat (do this ONCE PER SESSION / when the next user job is DIFFERENT from the prior user's job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c543e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_user_chat(\n",
    "                        task_folder='saved_chats', # The top level folder, where user chats are located\n",
    "                        user_output_folder='test_user', # The next level folder that stores all of a user's chats (define user names here)\n",
    "                        saved_chats_topic_name='ey_lmao1', # The bottom level folder that stores a specific chat from the user (contains .json / .pkl files) \n",
    "    ):\n",
    "\n",
    "    directory = f'user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Define the name of the chat, and the .json / .pkl files that will be saved with it\n",
    "    chat_json_name = f\"{saved_chats_topic_name}.json\"\n",
    "    chat_embedded_history_name = 'user_embedded_history.pkl'\n",
    "    chat_non_embedded_history_name = 'user_non_embedded_history.pkl'\n",
    "\n",
    "    # Define the path of the json / pkl files\n",
    "    final_json_path = os.path.join(directory, chat_json_name)\n",
    "    final_embedded_history_path = os.path.join(directory, chat_embedded_history_name)\n",
    "    final_non_embedded_history_path = os.path.join(directory, chat_non_embedded_history_name)\n",
    "\n",
    "    # Load the chat history given these paths\n",
    "    user_query_state_history, query_num, HISTORICAL_QUERY_NUM = load_query_history(QDRANT_CLIENT=QDRANT_CLIENT, \n",
    "            HISTORY_COLLECTION=HISTORY_COLLECTION,\n",
    "            chat_embedded_history_path=final_embedded_history_path,\n",
    "            chat_non_embedded_history_path=final_non_embedded_history_path\n",
    "            )\n",
    "\n",
    "    initialized_chat_settings = {\n",
    "        'user_chat_json_path': final_json_path,\n",
    "        'user_embedded_history_path': final_embedded_history_path,\n",
    "        'user_non_embedded_history_path': final_non_embedded_history_path,\n",
    "        'query_num': query_num,\n",
    "        'historical_query_num': HISTORICAL_QUERY_NUM,\n",
    "        'user_query_state_history': user_query_state_history\n",
    "    }\n",
    "\n",
    "    return initialized_chat_settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f37ebc",
   "metadata": {},
   "source": [
    "## Call Response Generation Function (DO THIS AS MANY TIMES AS YOU WANT PER SESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c923f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_back_end_response_generation(\n",
    "                                      user_query_state_history: dict,\n",
    "                                      desired_history_window_size: int,\n",
    "                                      desired_context_chunks_top_k: int,\n",
    "                                      rag_switch: bool,\n",
    "                                      history_switch: bool,\n",
    "                                      bm25_switch: bool,\n",
    "                                      topic_retrieval_switch: bool,\n",
    "                                      historic_query_similarity_threshold: float, # [0, 1] range (filter)\n",
    "                                      query_text: str,\n",
    "                                      query_num: int,\n",
    "    ):\n",
    "\n",
    "    # Call the `process_query()` function with the inputs\n",
    "    user_query_state_history[query_num] = process_query(\n",
    "        desired_history_window_size, \n",
    "        desired_context_chunks_top_k, \n",
    "        rag_switch, \n",
    "        history_switch, \n",
    "        bm25_switch, \n",
    "        topic_retrieval_switch, \n",
    "        historic_query_similarity_threshold, \n",
    "        query_text, \n",
    "        user_query_state_history,\n",
    "        query_num, \n",
    "        QDRANT_CLIENT, \n",
    "        CHUNK_COLLECTION,\n",
    "        HISTORY_COLLECTION,\n",
    "        LLM_MODEL,\n",
    "        LLM_SYSTEM_PROMPT,\n",
    "        DEVICE,\n",
    "        EMBEDDING_MODEL, \n",
    "        TOKENIZER,\n",
    "        BM25_SEARCH_FUNCTION\n",
    "    )\n",
    "\n",
    "    return user_query_state_history[query_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d35e9c",
   "metadata": {},
   "source": [
    "## Input a Query to the Backend (DO THIS AS MANY TIMES AS YOU WANT PER SESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_query(\n",
    "                user_chat_settings: dict={}, # the initial settings used to route / initialize the chat  \n",
    "                frontend_inputs: dict={},\n",
    "                end_session: bool=False # the user inputs for a given query                  \n",
    "    ):\n",
    "\n",
    "    MY_USER_JSON_PATH = user_chat_settings['user_chat_json_path']\n",
    "\n",
    "    query_num = user_chat_settings['query_num']\n",
    "    #HISTORICAL_QUERY_NUM = user_chat_settings['historical_query_num']\n",
    "    user_query_state_history = user_chat_settings['user_query_state_history']\n",
    "\n",
    "    USER_INPUT_QUERY = frontend_inputs['USER_INPUT_QUERY']\n",
    "    DESIRED_HISTORY_WINDOW_SIZE = frontend_inputs['DESIRED_HISTORY_WINDOW_SIZE']\n",
    "    DESIRED_CONTEXT_CHUNKS_TOP_K = frontend_inputs['DESIRED_CONTEXT_CHUNKS_TOP_K']\n",
    "    RAG_SWITCH = frontend_inputs['RAG_SWITCH']\n",
    "    HISTORY_SWITCH = frontend_inputs['HISTORY_SWITCH']\n",
    "    BM25_SWITCH = frontend_inputs['BM25_SWITCH']\n",
    "    TOPIC_RETRIEVAL_SWITCH = frontend_inputs['TOPIC_RETRIEVAL_SWITCH']\n",
    "    HISTORIC_QUERY_SIMILARITY_THRESHOLD = frontend_inputs['HISTORIC_QUERY_SIMILARITY_THRESHOLD']\n",
    "\n",
    "    # Increment the query by 1\n",
    "    query_num += 1\n",
    "\n",
    "    user_query_state_history[query_num] = call_back_end_response_generation(\n",
    "                        user_query_state_history=user_query_state_history,\n",
    "                        desired_history_window_size=DESIRED_HISTORY_WINDOW_SIZE,\n",
    "                        desired_context_chunks_top_k=DESIRED_CONTEXT_CHUNKS_TOP_K,\n",
    "                        rag_switch=RAG_SWITCH,\n",
    "                        history_switch=HISTORY_SWITCH,\n",
    "                        bm25_switch=BM25_SWITCH,\n",
    "                        topic_retrieval_switch=TOPIC_RETRIEVAL_SWITCH,\n",
    "                        historic_query_similarity_threshold=HISTORIC_QUERY_SIMILARITY_THRESHOLD,\n",
    "                        query_text=USER_INPUT_QUERY,\n",
    "                        query_num=query_num\n",
    "    )\n",
    "\n",
    "    user_chat_settings['user_query_state_history'][query_num] = user_query_state_history[query_num]\n",
    "    user_chat_settings['query_num'] = query_num\n",
    "\n",
    "    save_chat_json(user_query_state_history[query_num], file_path=MY_USER_JSON_PATH)\n",
    "    \n",
    "    print(f\"User State Length: {len(user_query_state_history)}\")\n",
    "    print(f\"\\nResponse: {user_query_state_history[query_num]['response_text']}\\n\")\n",
    "\n",
    "    return user_chat_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e43349",
   "metadata": {},
   "source": [
    "## Save the Chat Session / Move to next user's job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67168c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_user_session(user_chat_settings):\n",
    "\n",
    "    # Save the PKL file at the end of the session\n",
    "    MY_USER_EMBEDDED_HISTORY_PATH = user_chat_settings['user_embedded_history_path']\n",
    "    MY_USER_NON_EMBEDDED_HISTORY_PATH = user_chat_settings['user_non_embedded_history_path']\n",
    "    USER_QUERY_STATE_HISTORY = user_chat_settings['user_query_state_history']\n",
    "\n",
    "    save_chat_pkl_by_embedding(\n",
    "        user_query_state_history=USER_QUERY_STATE_HISTORY,\n",
    "        embedded_path=MY_USER_EMBEDDED_HISTORY_PATH,\n",
    "        non_embedded_path=MY_USER_NON_EMBEDDED_HISTORY_PATH\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8a95",
   "metadata": {},
   "source": [
    "# Initialize For a Single Session:\n",
    "\n",
    "### Below are steps that need to be taken to initialize and process user inputs in a **single session**, utillizing the functions defined above in the `**Setup**` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c2e60",
   "metadata": {},
   "source": [
    "## Define user's directory to route history\n",
    "\n",
    "This can handle both new and existing chats, it serves as a method of routing queries to storage spaces as well as routing where to save session information!\n",
    "\n",
    "All output is stored in the `**user_output**` folder; after that there are 3 levels that you can work with:\n",
    "\n",
    "1. `user_chat_group`: the folder that indicates the **type** of the chat group\n",
    "2. `test_user`: the folder that inidcates the actual **user's id / name**\n",
    "3. `user_chat_topic`: the folder that stores a specific session for a specific user. stores `.pkl` and `.json` files "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3a61ff81",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd25459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat, start from query number: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'test_user'\n",
    "user_chat_topic = 'ey_lmao25'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f8d49",
   "metadata": {},
   "source": [
    "## Input Queries during the Session:\n",
    "\n",
    "Now that we have defined our environment, and our session routing information, we can being inputting queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88110b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Chunk ID: 6661, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 354, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2261, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 657, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4087, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 637, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2264, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 672, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4109, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 599, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "Final User State Length: 4\n",
      "\n",
      "Response: Hello! I am doing well, thank you for asking! I am ready to provide you with detailed and comprehensive information based on your requests, drawing upon insights from AI/ML research papers.\n",
      "\n",
      "\n",
      "#### Chunk ID: 4005, Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement\n",
      "  Learning Algorithm, Publish Date: 2017-02 , Link: https://arxiv.org/abs/1712.01815, # of Tokens: 720, Section: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm ---> **Methods** ---> **Example games** ---> 17\n",
      "#### Chunk ID: 4525, Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement\n",
      "  Learning Algorithm, Publish Date: 2017-02 , Link: https://arxiv.org/abs/1712.01815, # of Tokens: 1389, Section: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm ---> **Methods** ---> **Example games** ---> 17\n",
      "Final User State Length: 5\n",
      "\n",
      "Response: Let's revisit the question \"Is the sky blue?\"\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "The question \"Is the sky blue?\" is a fundamental example of a query that tests a machine's ability to recall factual knowledge. As noted in prior responses, this falls under the umbrella of question-answering (QA) within NLP. An AI model, particularly a large language model (LLM), leverages its training data to provide a direct and accurate answer. The simplicity of the question belies its importance in understanding how AI models acquire and apply knowledge.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "Answering \"Is the sky blue?\" involves the following key elements, echoing the principles previously established:\n",
      "\n",
      "*   **Knowledge Retrieval:** The model accesses its pre-existing knowledge about the world, learned through training on vast text datasets.\n",
      "*   **Question Understanding:** The model parses the question to understand its intent and context.\n",
      "*   **Answer Generation:** The model formulates a response based on its internal understanding and knowledge.\n",
      "\n",
      "In essence, the model retrieves the fact that the sky is blue from its internal representation of the world. Context Chunk 1's examples directly demonstrate this capability in action.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "The context chunks provide clear examples to illustrate this point. Context Chunk 1, for instance, contains a conversation where the human asks \"is sky blue or black?\" and the machine answers \"blue.\" This simple interaction showcases the model's ability to respond correctly to a direct question. Further examples from this chunk also demonstrate the models' ability to answer general knowledge questions like \"What is the color of the sky?\" with \"blue.\"\n",
      "\n",
      "Context Chunk 0 extends this by showing the same question in multiple languages. This demonstrates how the LLMs can perform cross-linguistic mapping, retrieving the same core knowledge regardless of the question's language.\n",
      "\n",
      "Context Chunk 2 does not directly provide an answer to the question, however, it does show a series of questions and their corresponding answers.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "To build upon this, here are some related areas for exploration:\n",
      "\n",
      "*   **Knowledge Representation and Reasoning:** Explore how AI models store and utilize knowledge. Consider different methods for organizing information, such as knowledge graphs, and investigate how these methods can improve question-answering capabilities.\n",
      "*   **Multilingual Question Answering:** Investigate how to create QA systems that can understand and answer questions in multiple languages. This includes understanding the challenges and techniques associated with cross-lingual knowledge retrieval and transfer.\n",
      "*   **Fact Verification and Counterfactual Reasoning**: Investigate the model's ability to distinguish between true and false statements, and the capability to hypothesize alternate scenarios based on available information.\n",
      "\n",
      "I hope this response furthers your understanding.\n",
      "\n",
      "\n",
      "#### Chunk ID: 6395, Title: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition, Publish Date: 2018-04 , Link: https://arxiv.org/abs/1804.03209, # of Tokens: 385, Section: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition ---> **5 Collection** ---> **5.3 Implementation**\n",
      "#### Chunk ID: 2949, Title: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition, Publish Date: 2018-04 , Link: https://arxiv.org/abs/1804.03209, # of Tokens: 543, Section: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition ---> **5 Collection** ---> **5.3 Implementation**\n",
      "Final User State Length: 6\n",
      "\n",
      "Response: Let's delve into the question, \"What does the ocean sound like?\"\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "The query \"What does the ocean sound like?\" is a fascinating example of a task for an AI model. It goes beyond simple factual recall, demanding the model to generate a description of a sensory experience. The model must leverage its knowledge of the ocean and its associated sounds, and then translate this into a descriptive, easily understandable narrative.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "To address this question, an AI model would need to combine several capabilities:\n",
      "\n",
      "*   **Knowledge Retrieval:** The model must access its learned information about the ocean. This knowledge is typically embedded within its parameters, acquired through training on vast text and, potentially, audio datasets.\n",
      "*   **Auditory Sensory Understanding:** The model needs to associate the concept of \"ocean\" with characteristic sounds.\n",
      "*   **Descriptive Language Generation:** It has to generate text that describes auditory experiences, using terms such as \"waves crashing,\" \"seabirds calling,\" or \"lapping water.\"\n",
      "*   **Natural Language Generation (NLG):** The model must formulate a coherent response that conveys the information in a natural and engaging manner.\n",
      "\n",
      "Given the provided context chunks, an AI model can utilize its question-answering abilities to address the current query. However, the model must also make inferences about the characteristics of the ocean based on its training data.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "Several context chunks showcase relevant concepts. Context Chunk 1 presents the model's capability for performing question-answering within a simple Q&A structure. This capability shows how the model can associate concepts with corresponding attributes. The model associates concepts like \"sky\" with the color \"blue\". In the current scenario, the model must connect the concept of \"ocean\" to associated sounds.\n",
      "\n",
      "Context Chunk 4 introduces the concept of background noise. This is not directly related to the sound of the ocean, but it does shed light on challenges in audio processing and the need to distinguish between desired and undesired audio signals.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "To further expand your understanding, consider the following related topics:\n",
      "\n",
      "*   **Audio Generation and Processing:** Explore how AI models are employed in generating and processing audio, including techniques for creating realistic soundscapes, sound identification, and audio enhancement.\n",
      "*   **Multimodal AI:** Investigate AI models that integrate multiple modalities (text, audio, visual information, etc.) to create more comprehensive and human-like understandings of the world. This is especially relevant when an AI is expected to offer details based on various sensory inputs.\n",
      "*   **Creative Writing with AI:** Examine the use of AI in creative writing, encompassing poetry, storytelling, and the generation of descriptive texts. This can offer insights into an AI's capacity to create narratives and descriptions based on prompts.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#### First Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_1,\n",
    ")\n",
    "\n",
    "#######################################\n",
    "#### Second Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_2 = {\n",
    "    'USER_INPUT_QUERY' : 'is the sky blue?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_2,\n",
    ")\n",
    "\n",
    "#######################################\n",
    "#### Final Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_3 = {\n",
    "    'USER_INPUT_QUERY' : 'what does the ocean sound like?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# When the user decides to end the session (e.g., after the last query)\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_3, \n",
    ")\n",
    "\n",
    "# End the current user's session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59efb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44266c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_number', 'query_text', 'query_embedding', 'rag_used', 'history_used', 'rag_and_history_used', 'bm25_used', 'topic_retrieval_used', 'desired_lookback_window_size', 'actual_lookback_window_size', 'desired_top_k_chunks', 'query_start_time', 'query_finish_time', 'query_processing_time', 'context_ids_utilized', 'context_ids_source', 'response_text', 'prompt_token_count', 'candidates_token_count', 'total_token_count', 'system_prompt_used', 'dynamic_prompt_body', 'current_state_context_ids', 'current_state_context_scores', 'retrieval_top_k', 'retrieval_method', 'avg_similarity_to_context', 'max_similarity_to_context', 'top_context_id', 'top_context_score', 'considered_prior_state_ids', 'utilized_prior_state_ids', 'filter_similarity_score_threshold', 'filter_similarity_score_mask', 'bm25_RRF_constant', 'bm25_multiplier', 'current_state_bm25_context_ids', 'current_state_bm25_context_scores', 'current_state_hybrid_fused_scores', 'top_topic_chunk_ids', 'top_topic_chunk_similarity', 'weight_coherence', 'weight_query_relevance', 'weight_chunk_count', 'experiment_tag'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2632dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_chat_query_num = list(user_chat_settings['user_query_state_history'].keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2f999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let\\'s delve into the question, \"What does the ocean sound like?\"\\n\\n### 1. Introductory Overview\\n\\nThe query \"What does the ocean sound like?\" is a fascinating example of a task for an AI model. It goes beyond simple factual recall, demanding the model to generate a description of a sensory experience. The model must leverage its knowledge of the ocean and its associated sounds, and then translate this into a descriptive, easily understandable narrative.\\n\\n### 2. Technical Overview\\n\\nTo address this question, an AI model would need to combine several capabilities:\\n\\n*   **Knowledge Retrieval:** The model must access its learned information about the ocean. This knowledge is typically embedded within its parameters, acquired through training on vast text and, potentially, audio datasets.\\n*   **Auditory Sensory Understanding:** The model needs to associate the concept of \"ocean\" with characteristic sounds.\\n*   **Descriptive Language Generation:** It has to generate text that describes auditory experiences, using terms such as \"waves crashing,\" \"seabirds calling,\" or \"lapping water.\"\\n*   **Natural Language Generation (NLG):** The model must formulate a coherent response that conveys the information in a natural and engaging manner.\\n\\nGiven the provided context chunks, an AI model can utilize its question-answering abilities to address the current query. However, the model must also make inferences about the characteristics of the ocean based on its training data.\\n\\n### 3. Example-Based Expansion\\n\\nSeveral context chunks showcase relevant concepts. Context Chunk 1 presents the model\\'s capability for performing question-answering within a simple Q&A structure. This capability shows how the model can associate concepts with corresponding attributes. The model associates concepts like \"sky\" with the color \"blue\". In the current scenario, the model must connect the concept of \"ocean\" to associated sounds.\\n\\nContext Chunk 4 introduces the concept of background noise. This is not directly related to the sound of the ocean, but it does shed light on challenges in audio processing and the need to distinguish between desired and undesired audio signals.\\n\\n### 4. Broader Exploration\\n\\nTo further expand your understanding, consider the following related topics:\\n\\n*   **Audio Generation and Processing:** Explore how AI models are employed in generating and processing audio, including techniques for creating realistic soundscapes, sound identification, and audio enhancement.\\n*   **Multimodal AI:** Investigate AI models that integrate multiple modalities (text, audio, visual information, etc.) to create more comprehensive and human-like understandings of the world. This is especially relevant when an AI is expected to offer details based on various sensory inputs.\\n*   **Creative Writing with AI:** Examine the use of AI in creative writing, encompassing poetry, storytelling, and the generation of descriptive texts. This can offer insights into an AI\\'s capacity to create narratives and descriptions based on prompts.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'][most_recent_chat_query_num]['response_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45241a04",
   "metadata": {},
   "source": [
    "# IMPORTANT - A Potential Guide to Handling Multiple Users / Scheduling\n",
    "\n",
    "If you want to swap to process different user queries; you will have to re-route before each input query. This means that if you want to serve multiple users, you have to save to set `**end_session=True**` for every job from every user, otherwise the qdrant store that handles searches against user queries will be populated by everybody's history at the same time. Furthermore, it will over-write itself because the query_ids that it utilizes (values such as `1`, `2`, `3`) will be overwritten by multiple users using the system at the same time.\n",
    "\n",
    "Handling this by creating a seperate collection for each user is not the worst solution at our scale, but making this dynamic would be a massive pain.\n",
    "\n",
    "## Scheduling Guide\n",
    "\n",
    "In order to handle multiple users; you process the first input query, and then hold their routing information / information until the next input query. If the next input query's source is different, we can load the new routing information and process their query. If the next input query's source is the same, we can simply process the query without having to re-load their information.\n",
    "\n",
    "### Variable Definitions:\n",
    "\n",
    "Each user has query routing information based on their UUID / their connection information:\n",
    "\n",
    "```python\n",
    "user_chat_group: str = ''\n",
    "chat_user: str = ''\n",
    "user_chat_topic: str = ''\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Each Job has information structured:\n",
    "\n",
    "```python\n",
    "job = {\n",
    "    'USER_INPUT_QUERY' : str,\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : int,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : int,\n",
    "    'RAG_SWITCH' : bool,\n",
    "    'HISTORY_SWITCH' : bool,\n",
    "    'BM25_SWITCH' : bool,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : bool,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : float,\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Say we have 2 users:\n",
    "\n",
    "our_unique_users = [`user_1`, `user_2`]\n",
    "\n",
    "<br>\n",
    "\n",
    "And they input jobs that can be sequentially ordered:\n",
    "\n",
    "`user_1`: `job_1`, `job_2`, `job_4`\n",
    "\n",
    "`user_2`: `job_3`, `job_5`\n",
    "\n",
    "<br>\n",
    "\n",
    "Where a schedule is represented like this:\n",
    "\n",
    "all_jobs_t0 =        [ `job_1`,  `job_2`,  `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t0 = [`user_1`, `user_1`, `user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Where `t0` refers to the time step.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Scheudling Example Code:\n",
    "\n",
    "#### Time step t=0:\n",
    "\n",
    "We start with `user_1` who has `job_1`. We have to load their routing information and process their job so that we can serve them with their response.\n",
    "\n",
    "``` python\n",
    "#######################################\n",
    "#### First Query: `job_1` from `user_1`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_1` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_1'\n",
    "user_chat_topic = 'user_1_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_1,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=1:\n",
    "\n",
    "Updated queue at time step 1:\n",
    "\n",
    "all_jobs_t1 =        [ `job_2`,  `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t1 = [`user_1`, `user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Nobody else inputs a query before `user_1` inputs `job_2`, so their data does not have to be permanently saved and reloaded in order to process `job_2`. \n",
    "\n",
    "```python\n",
    "#######################################\n",
    "#### Second Query `job_2` from `user_1`\n",
    "#######################################\n",
    "\n",
    "job_2 = {\n",
    "    'USER_INPUT_QUERY' : 'is the sky blue?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=job_2,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=2:\n",
    "\n",
    "Updated queue at time step 2:\n",
    "\n",
    "all_jobs_t2 =        [ `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t2 = [`user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Now, `user_2` is the owner of the next job, `job_3`. However, we are still holding `user_1` routing information; we have to end their session and permanently save their session information. After this, we can get `user_2` routing information in to process `job_3`.\n",
    "\n",
    "```python\n",
    "# End `user_1` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Third Query `job_3` from `user_2`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_2` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_2'\n",
    "user_chat_topic = 'user_2_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_3 = {\n",
    "    'USER_INPUT_QUERY' : 'what does the ocean sound like?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# When the user decides to end the session (e.g., after the last query)\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=job_3, \n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=3:\n",
    "\n",
    "Updated queue at time step 3:\n",
    "\n",
    "all_jobs_t3 =        [ `job_4`, `job_5`]\n",
    "all_jobs_source_t3 = [`user_1`,`user_2`]\n",
    "\n",
    "Now, `user_1` has `job_4`. We need to end `user_2` session and save their information to process `job_4`.\n",
    "\n",
    "```python\n",
    "# End `user_2` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Fourth Query `job_4` from `user_1`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_1` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_1'\n",
    "user_chat_topic = 'user_1_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_4 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_4,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=4:\n",
    "\n",
    "Updated queue at time step 4:\n",
    "\n",
    "all_jobs_t4 =        [ `job_5`]\n",
    "all_jobs_source_t4 = [`user_2`]\n",
    "\n",
    "Finally, `user_2` inputs `job_5`. We permanently save `user_1` information and route to `user_2`.\n",
    "\n",
    "```python\n",
    "# End `user_1` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Fifth Query `job_5` from `user_2`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_2` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_2'\n",
    "user_chat_topic = 'user_2_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_5 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_5,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Lastly, we hold of on saving permanent history until the next job enters our queue. If the user who we just served (in this case, `user_2`) decides to close out of the application or start a new chat, trigger a permanent save of their last chat and keep waiting for the next job. \n",
    "\n",
    "And now we have completed all the scheduled jobs, while maintaining a permanent session history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833716bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
