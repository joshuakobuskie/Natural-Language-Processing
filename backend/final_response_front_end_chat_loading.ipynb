{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5b378c",
   "metadata": {},
   "source": [
    "# Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac86417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the uploaded files\n",
    "import os\n",
    "from final_response_front_end_main import initialize_system, process_query\n",
    "from final_response_front_end_main import load_query_history\n",
    "from final_response_front_end_main import flush_prior_qdrant_client_and_initialize_new_client\n",
    "from user_history_utils import save_chat_pkl_by_embedding\n",
    "from user_history_utils import save_chat_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a83266",
   "metadata": {},
   "source": [
    "# Setups:\n",
    "\n",
    "### Below are the various setups of environment variables and functions that are required for **EVERY SESSION**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f115c67",
   "metadata": {},
   "source": [
    "## Intialize System Environment (do this once PER BOOT OF SYSTEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953ab835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "Loading Snowflake/snowflake-arctic-embed-l-v2.0 from local storage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at qdrant_vector_store/local_embedding_models/Snowflake/snowflake-arctic-embed-l-v2.0 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snowflake/snowflake-arctic-embed-l-v2.0 embedding model loaded to cuda\n",
      "\n",
      "\n",
      "############### System Prompt:\n",
      "\n",
      "    You are an advanced AI research assistant. Generate detailed and comprehensive responses that supplement students' and academic researchers' work with information grounded in highly cited AI/ML research papers, specifically in fields like NLP and CV. The response should not focus on one area of study but should be informed by both the current query and chat history to generate a well-rounded answer.\n",
      "\n",
      "    1. **Introductory Overview**: Start with a high-level conceptual overview of the topic, providing a brief and clear explanation that covers the essential aspects of the subject. This should be accessible to a broad audience.\n",
      "\n",
      "    2. **Technical Overview**: After the conceptual overview, provide a more in-depth, technical explanation that dives deeper into the topic. This could include relevant algorithms, methods, or models, as well as their theoretical foundations.\n",
      "\n",
      "    3. **Example-Based Expansion**: Throughout the response, incorporate examples from relevant research to illustrate key concepts. These examples should come from generalized research trends and not focus on specific papers or studies, helping to broaden the context.\n",
      "\n",
      "    4. **Broader Exploration**: After addressing the original query, provide suggestions for related topics or areas for further exploration, encouraging the user to expand their understanding. The exploration should relate to the current query and prior query/response pairs, offering natural extensions to the discussion, such as other approaches, applications, or advancements related to the topic.\n",
      "\n",
      "    The tone should be professional yet approachable, offering a balance of conceptual clarity and technical depth. The response should not be overly simplistic, but should aim to make complex topics understandable while offering substantial detail. Use direct quotes where relevant, but focus primarily on summarizing findings from academic research.\n",
      "    \n",
      "Upserted batch of 10443 points to qdrant client collection search_collection\n",
      "Loading chat, start from query number: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "DEVICE, TOKENIZER, EMBEDDING_MODEL, LLM_MODEL, LLM_SYSTEM_PROMPT, QDRANT_CLIENT, CHUNK_COLLECTION, HISTORY_COLLECTION, BM25_SEARCH_FUNCTION, _, _, _ = initialize_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d02dae",
   "metadata": {},
   "source": [
    "## Initialize a user's chat (do this ONCE PER SESSION / when the next user job is DIFFERENT from the prior user's job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81c543e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_user_chat(\n",
    "                        task_folder='saved_chats', # The top level folder, where user chats are located\n",
    "                        user_output_folder='test_user', # The next level folder that stores all of a user's chats (define user names here)\n",
    "                        saved_chats_topic_name='ey_lmao1', # The bottom level folder that stores a specific chat from the user (contains .json / .pkl files) \n",
    "    ):\n",
    "\n",
    "    directory = f'user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Define the name of the chat, and the .json / .pkl files that will be saved with it\n",
    "    chat_json_name = f\"{saved_chats_topic_name}.json\"\n",
    "    chat_embedded_history_name = 'user_embedded_history.pkl'\n",
    "    chat_non_embedded_history_name = 'user_non_embedded_history.pkl'\n",
    "\n",
    "    # Define the path of the json / pkl files\n",
    "    final_json_path = os.path.join(directory, chat_json_name)\n",
    "    final_embedded_history_path = os.path.join(directory, chat_embedded_history_name)\n",
    "    final_non_embedded_history_path = os.path.join(directory, chat_non_embedded_history_name)\n",
    "\n",
    "    # Load the chat history given these paths\n",
    "    user_query_state_history, query_num, HISTORICAL_QUERY_NUM = load_query_history(QDRANT_CLIENT=QDRANT_CLIENT, \n",
    "            HISTORY_COLLECTION=HISTORY_COLLECTION,\n",
    "            chat_embedded_history_path=final_embedded_history_path,\n",
    "            chat_non_embedded_history_path=final_non_embedded_history_path\n",
    "            )\n",
    "\n",
    "    initialized_chat_settings = {\n",
    "        'user_chat_json_path': final_json_path,\n",
    "        'user_embedded_history_path': final_embedded_history_path,\n",
    "        'user_non_embedded_history_path': final_non_embedded_history_path,\n",
    "        'query_num': query_num,\n",
    "        'historical_query_num': HISTORICAL_QUERY_NUM,\n",
    "        'user_query_state_history': user_query_state_history\n",
    "    }\n",
    "\n",
    "    return initialized_chat_settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f37ebc",
   "metadata": {},
   "source": [
    "## Call Response Generation Function (DO THIS AS MANY TIMES AS YOU WANT PER SESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c923f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_back_end_response_generation(\n",
    "                                      user_query_state_history: dict,\n",
    "                                      desired_history_window_size: int,\n",
    "                                      desired_context_chunks_top_k: int,\n",
    "                                      rag_switch: bool,\n",
    "                                      history_switch: bool,\n",
    "                                      bm25_switch: bool,\n",
    "                                      topic_retrieval_switch: bool,\n",
    "                                      historic_query_similarity_threshold: float, # [0, 1] range (filter)\n",
    "                                      query_text: str,\n",
    "                                      query_num: int,\n",
    "    ):\n",
    "\n",
    "    # Call the `process_query()` function with the inputs\n",
    "    user_query_state_history[query_num] = process_query(\n",
    "        desired_history_window_size, \n",
    "        desired_context_chunks_top_k, \n",
    "        rag_switch, \n",
    "        history_switch, \n",
    "        bm25_switch, \n",
    "        topic_retrieval_switch, \n",
    "        historic_query_similarity_threshold, \n",
    "        query_text, \n",
    "        user_query_state_history,\n",
    "        query_num, \n",
    "        QDRANT_CLIENT, \n",
    "        CHUNK_COLLECTION,\n",
    "        HISTORY_COLLECTION,\n",
    "        LLM_MODEL,\n",
    "        LLM_SYSTEM_PROMPT,\n",
    "        DEVICE,\n",
    "        EMBEDDING_MODEL, \n",
    "        TOKENIZER,\n",
    "        BM25_SEARCH_FUNCTION\n",
    "    )\n",
    "\n",
    "    return user_query_state_history[query_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d35e9c",
   "metadata": {},
   "source": [
    "## Input a Query to the Backend (DO THIS AS MANY TIMES AS YOU WANT PER SESSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c1c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_query(\n",
    "                user_chat_settings: dict={}, # the initial settings used to route / initialize the chat  \n",
    "                frontend_inputs: dict={},\n",
    "    ):\n",
    "\n",
    "    MY_USER_JSON_PATH = user_chat_settings['user_chat_json_path']\n",
    "\n",
    "    query_num = user_chat_settings['query_num']\n",
    "    #HISTORICAL_QUERY_NUM = user_chat_settings['historical_query_num']\n",
    "    user_query_state_history = user_chat_settings['user_query_state_history']\n",
    "\n",
    "    USER_INPUT_QUERY = frontend_inputs['USER_INPUT_QUERY']\n",
    "    DESIRED_HISTORY_WINDOW_SIZE = frontend_inputs['DESIRED_HISTORY_WINDOW_SIZE']\n",
    "    DESIRED_CONTEXT_CHUNKS_TOP_K = frontend_inputs['DESIRED_CONTEXT_CHUNKS_TOP_K']\n",
    "    RAG_SWITCH = frontend_inputs['RAG_SWITCH']\n",
    "    HISTORY_SWITCH = frontend_inputs['HISTORY_SWITCH']\n",
    "    BM25_SWITCH = frontend_inputs['BM25_SWITCH']\n",
    "    TOPIC_RETRIEVAL_SWITCH = frontend_inputs['TOPIC_RETRIEVAL_SWITCH']\n",
    "    HISTORIC_QUERY_SIMILARITY_THRESHOLD = frontend_inputs['HISTORIC_QUERY_SIMILARITY_THRESHOLD']\n",
    "\n",
    "    # Increment the query by 1\n",
    "    query_num += 1\n",
    "\n",
    "    user_query_state_history[query_num] = call_back_end_response_generation(\n",
    "                        user_query_state_history=user_query_state_history,\n",
    "                        desired_history_window_size=DESIRED_HISTORY_WINDOW_SIZE,\n",
    "                        desired_context_chunks_top_k=DESIRED_CONTEXT_CHUNKS_TOP_K,\n",
    "                        rag_switch=RAG_SWITCH,\n",
    "                        history_switch=HISTORY_SWITCH,\n",
    "                        bm25_switch=BM25_SWITCH,\n",
    "                        topic_retrieval_switch=TOPIC_RETRIEVAL_SWITCH,\n",
    "                        historic_query_similarity_threshold=HISTORIC_QUERY_SIMILARITY_THRESHOLD,\n",
    "                        query_text=USER_INPUT_QUERY,\n",
    "                        query_num=query_num\n",
    "    )\n",
    "\n",
    "    user_chat_settings['user_query_state_history'][query_num] = user_query_state_history[query_num]\n",
    "    user_chat_settings['query_num'] = query_num\n",
    "\n",
    "    save_chat_json(user_query_state_history[query_num], file_path=MY_USER_JSON_PATH)\n",
    "    \n",
    "    print(f\"User State Length: {len(user_query_state_history)}\")\n",
    "    print(f\"\\nResponse: {user_query_state_history[query_num]['response_text']}\\n\")\n",
    "\n",
    "    return user_chat_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e43349",
   "metadata": {},
   "source": [
    "## Save the Chat Session / Move to next user's job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67168c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_user_session(user_chat_settings):\n",
    "\n",
    "    # Save the PKL file at the end of the session\n",
    "    MY_USER_EMBEDDED_HISTORY_PATH = user_chat_settings['user_embedded_history_path']\n",
    "    MY_USER_NON_EMBEDDED_HISTORY_PATH = user_chat_settings['user_non_embedded_history_path']\n",
    "    USER_QUERY_STATE_HISTORY = user_chat_settings['user_query_state_history']\n",
    "\n",
    "    #flush_qdrant_client(user_id) # to flush the current user's history collection \n",
    "\n",
    "    save_chat_pkl_by_embedding(\n",
    "        user_query_state_history=USER_QUERY_STATE_HISTORY,\n",
    "        embedded_path=MY_USER_EMBEDDED_HISTORY_PATH,\n",
    "        non_embedded_path=MY_USER_NON_EMBEDDED_HISTORY_PATH\n",
    "    )\n",
    "\n",
    "    flush_prior_qdrant_client_and_initialize_new_client(client=QDRANT_CLIENT, EMBEDDING_MODEL=EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f8a95",
   "metadata": {},
   "source": [
    "# Initialize For a Single Session:\n",
    "\n",
    "### Below are steps that need to be taken to initialize and process user inputs in a **single session**, utillizing the functions defined above in the `**Setup**` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c2e60",
   "metadata": {},
   "source": [
    "## Define user's directory to route history\n",
    "\n",
    "This can handle both new and existing chats, it serves as a method of routing queries to storage spaces as well as routing where to save session information!\n",
    "\n",
    "All output is stored in the `**user_output**` folder; after that there are 3 levels that you can work with:\n",
    "\n",
    "1. `user_chat_group`: the folder that indicates the **type** of the chat group\n",
    "2. `test_user`: the folder that inidcates the actual **user's id / name**\n",
    "3. `user_chat_topic`: the folder that stores a specific session for a specific user. stores `.pkl` and `.json` files "
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "3a61ff81",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd25459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat, start from query number: 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'test_user'\n",
    "user_chat_topic = 'ey_lmao25'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f8d49",
   "metadata": {},
   "source": [
    "## Input Queries during the Session:\n",
    "\n",
    "Now that we have defined our environment, and our session routing information, we can being inputting queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cfebcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Chunk ID: 6661, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 354, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2261, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 657, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4087, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 637, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2264, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 672, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4109, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 599, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "User State Length: 8\n",
      "\n",
      "Response: Let's build upon our previous discussion. The query \"Hello, how are you today?\" is a typical conversational opener. I can respond and maintain the persona I am designed to have.\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "The query \"Hello, how are you today?\" serves as a starting point for a dialogue, functioning as a social greeting. Responding appropriately requires that the AI understand it's a greeting, recognize the conversational flow, and generate an engaging response consistent with its established persona. This reflects the importance of maintaining a conversational flow.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "To effectively respond to a greeting like \"Hello, how are you today?\", the AI needs:\n",
      "\n",
      "*   **Contextual Awareness**: Recognize that the phrase initiates a conversation.\n",
      "*   **Persona Consistency**: Maintain a consistent persona.\n",
      "*   **Natural Language Generation (NLG)**: Construct a response that is both friendly and informative.\n",
      "\n",
      "The context chunks demonstrate how AI models can manage conversations, answer questions, and handle user interactions, all of which are relevant to answering this type of prompt.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "Context Chunk 1 showcases a helpful assistant. The model answers questions and responds to prompts, showing its capacity to engage in a conversation. This chunk shows examples of the model responding and engaging in a conversation, including asking the user if there is anything else to help with.\n",
      "\n",
      "Context Chunk 3 provides further examples of the model interacting with users to help solve password issues. This demonstrates the model's capacity to have a conversation to address a user's queries and provide helpful responses.\n",
      "\n",
      "Context Chunk 0 shows examples of the model in the form of a friendly AI assistant, demonstrating its capacity to engage in a conversation and respond to various user inquiries.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "Here are some directions you might consider exploring further:\n",
      "\n",
      "*   **Dialogue State Tracking**: Examine how AI models keep track of the state of a conversation, including user intents, previous turns, and context, to generate relevant responses.\n",
      "*   **Reinforcement Learning in Dialogue Systems**: Explore how reinforcement learning can be used to train AI models to optimize their conversational abilities, such as generating more engaging and helpful responses.\n",
      "*   **Emotionally Aware Dialogue**: Investigate how AI models can detect and respond to emotions in user input, and how this influences their responses.\n",
      "\n",
      "I hope this response has been useful.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#### First Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12aa878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=8 segments_count=1 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "collection_info = QDRANT_CLIENT.get_collection(\"history_collection\")\n",
    "\n",
    "print(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb583b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection \"history_collection\" has been deleted; new collection \"history_collection\" has been instantiated.\n"
     ]
    }
   ],
   "source": [
    "end_user_session(user_chat_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5592037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x75b708bcd100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QDRANT_CLIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c16a4bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=0 segments_count=1 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=1024, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=None, sharding_method=None, replication_factor=None, write_consistency_factor=None, read_fan_out_factor=None, on_disk_payload=None, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=None, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "collection_info = QDRANT_CLIENT.get_collection(\"history_collection\")\n",
    "\n",
    "print(collection_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc70ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Chunk ID: 6661, Title: Training language models to follow instructions with human feedback, Publish Date: 2022-03 , Link: https://arxiv.org/abs/2203.02155, # of Tokens: 594, Section: **Training language models to follow instructions** **with human feedback** ---> **A Additional prompt data details**\n",
      "#### Chunk ID: 2261, Title: A Neural Conversational Model, Publish Date: 2015-06 , Link: https://arxiv.org/abs/1506.05869, # of Tokens: 503, Section: **A Neural Conversational Model** ---> **5. Experiments**\n",
      "#### Chunk ID: 4087, Title: Training language models to follow instructions with human feedback, Publish Date: 2022-03 , Link: https://arxiv.org/abs/2203.02155, # of Tokens: 607, Section: **Training language models to follow instructions** **with human feedback** ---> **A Additional prompt data details**\n",
      "#### Chunk ID: 2264, Title: A Neural Conversational Model, Publish Date: 2015-06 , Link: https://arxiv.org/abs/1506.05869, # of Tokens: 597, Section: **A Neural Conversational Model** ---> **5. Experiments**\n",
      "#### Chunk ID: 4109, Title: A Neural Conversational Model, Publish Date: 2015-06 , Link: https://arxiv.org/abs/1506.05869, # of Tokens: 620, Section: **A Neural Conversational Model** ---> **5. Experiments**\n",
      "User State Length: 8\n",
      "\n",
      "Response: Let's address your current query, \"hello, how are you today?\"\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "As discussed in prior responses, this question initiates a conversation and tests an AI model's ability to engage in human-like interactions. The model needs to understand the greeting (\"hello\") and the inquiry about well-being (\"how are you today?\"). This requires the model to use Natural Language Understanding (NLU) to interpret the input and Natural Language Generation (NLG) to produce an appropriate response. This demonstrates the core function of any conversational AI or chatbot.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "An AI model capable of responding appropriately to \"Hello, how are you today?\" follows a process similar to the one established earlier, with the main components being:\n",
      "\n",
      "*   **Tokenization and Parsing:** The input is broken down into individual tokens (\"hello\", \"how\", \"are\", \"you\", \"today?\"). The model then parses these tokens to understand their grammatical relationships.\n",
      "*   **Intent Recognition:** The model identifies the intent of the user, which in this case is a greeting and a request for self-reporting.\n",
      "*   **Contextual Understanding:** The model considers the ongoing conversation, including the history of the dialogue and the user's prior inputs.\n",
      "*   **Response Generation:** Based on its internal state and the parsed query, the model generates a suitable response. This often involves retrieving information from its knowledge base (e.g., \"I am fine, thank you.\") and formulating a coherent sentence.\n",
      "\n",
      "The ability to perform this requires a model to have been trained on vast datasets of text and dialogue, which provides the AI model with knowledge about how humans communicate.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "The context chunks provided offer a variety of relevant examples. The context chunks presented showcase a range of conversational interactions that illustrate how AI models can respond to a greeting. For example, **Context Chunk 3** shows a dialogue exchange in which a chatbot greets a user and engages in a conversation. Also, **Context Chunk 1** shows a variety of simple interactions. These examples demonstrate how models manage the nuances of language, which includes greetings.\n",
      "\n",
      "Context Chunk 4 provides a more complex conversation; however, it shows the limitations of the AI's ability to respond.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "Building on this, there are several avenues for further exploration:\n",
      "\n",
      "*   **Reinforcement Learning for Dialogue:** Investigate how reinforcement learning can be used to train dialogue models. These models can learn to optimize responses based on rewards, leading to more engaging and effective conversations.\n",
      "*   **Persona-Based Chatbots:** Explore the creation of chatbots with specific personalities. This involves training models to generate responses that are consistent with a predefined persona.\n",
      "*   **Emotion Recognition and Response Generation:** Delve into techniques for detecting emotions in text and generating responses that are emotionally appropriate. This is crucial for creating AI models that can respond empathetically.\n",
      "\n",
      "Given your interest in conversational AI, consider also exploring the principles of user interface design, since effective interfaces are crucial for good user experience.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : False,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3579d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat, start from query number: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'test_user2'\n",
    "user_chat_topic = 'ey_lmao2'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60895de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History was meant to be utilized, but the window_size is 0, so no lookback will be performed (window size == 0, no history requested).\n",
      "#### Chunk ID: 6661, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 354, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2261, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 657, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4087, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 637, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2264, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 672, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4109, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 599, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "User State Length: 1\n",
      "\n",
      "Response: I am doing well, thank you for asking! How can I assist you today?\n",
      "\n",
      "Based on the provided context, it seems that the AI has access to conversational models and can engage in dialogues.\n",
      "If you have any questions or topics you'd like to discuss, please feel free to ask!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#### First Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fe88430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat, start from query number: 18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'test_user'\n",
    "user_chat_topic = 'ey_lmao2'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3be1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chat, start from query number: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'test_user2'\n",
    "user_chat_topic = 'ey_lmao2'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad3f848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_chat_json_path': 'user_output/saved_chats/test_user2/ey_lmao2/ey_lmao2.json',\n",
       " 'user_embedded_history_path': 'user_output/saved_chats/test_user2/ey_lmao2/user_embedded_history.pkl',\n",
       " 'user_non_embedded_history_path': 'user_output/saved_chats/test_user2/ey_lmao2/user_non_embedded_history.pkl',\n",
       " 'query_num': 1,\n",
       " 'historical_query_num': 0,\n",
       " 'user_query_state_history': {1: {'query_number': 1,\n",
       "   'query_text': 'hello, how are you today?',\n",
       "   'query_embedding': [-0.033832915127277374,\n",
       "    0.008098253980278969,\n",
       "    -0.03232055902481079,\n",
       "    -0.056461550295352936,\n",
       "    -0.028975700959563255,\n",
       "    0.004647496622055769,\n",
       "    0.0447278693318367,\n",
       "    0.039499253034591675,\n",
       "    -0.01821332797408104,\n",
       "    -0.036969419568777084,\n",
       "    -0.005970797501504421,\n",
       "    0.02120761200785637,\n",
       "    0.03863900899887085,\n",
       "    -0.00940358079969883,\n",
       "    0.009253262542188168,\n",
       "    0.07733768969774246,\n",
       "    -0.02736302837729454,\n",
       "    -0.0032472817692905664,\n",
       "    -0.05290427431464195,\n",
       "    -0.04829259589314461,\n",
       "    -0.0762454941868782,\n",
       "    0.06518201529979706,\n",
       "    0.0024886522442102432,\n",
       "    0.004769285209476948,\n",
       "    0.04688597097992897,\n",
       "    -0.002806853037327528,\n",
       "    -0.022589940577745438,\n",
       "    0.000726736499927938,\n",
       "    -0.01254058163613081,\n",
       "    -0.0025171013548970222,\n",
       "    -0.03681415319442749,\n",
       "    0.04175600782036781,\n",
       "    0.01943098194897175,\n",
       "    -0.0900898203253746,\n",
       "    -0.04456505924463272,\n",
       "    0.0024638071190565825,\n",
       "    0.051549479365348816,\n",
       "    -0.024446191266179085,\n",
       "    -0.1513075977563858,\n",
       "    0.02906467579305172,\n",
       "    -0.008338258601725101,\n",
       "    -0.0009743712143972516,\n",
       "    -0.039472054690122604,\n",
       "    0.03500247001647949,\n",
       "    0.050705697387456894,\n",
       "    -0.028954045847058296,\n",
       "    -0.03796159476041794,\n",
       "    -0.05359594523906708,\n",
       "    0.012957771308720112,\n",
       "    -0.029851140454411507,\n",
       "    0.016354534775018692,\n",
       "    0.040515054017305374,\n",
       "    -0.03386688604950905,\n",
       "    0.007973892614245415,\n",
       "    -0.041429441422224045,\n",
       "    0.017166342586278915,\n",
       "    0.003042373340576887,\n",
       "    -0.018168454989790916,\n",
       "    -0.05400489643216133,\n",
       "    0.0072903442196547985,\n",
       "    0.04918825626373291,\n",
       "    0.03488302230834961,\n",
       "    -0.011173522099852562,\n",
       "    0.03360495716333389,\n",
       "    -0.06982944160699844,\n",
       "    0.09496362507343292,\n",
       "    0.04069184511899948,\n",
       "    0.026029497385025024,\n",
       "    -0.03309103101491928,\n",
       "    -0.0026031644083559513,\n",
       "    -0.06210676580667496,\n",
       "    -0.007621017750352621,\n",
       "    0.003453665180131793,\n",
       "    0.06319009512662888,\n",
       "    -0.04700068384408951,\n",
       "    -0.055131297558546066,\n",
       "    0.02115272358059883,\n",
       "    0.019017979502677917,\n",
       "    0.03402566537261009,\n",
       "    0.03306533023715019,\n",
       "    0.17310921847820282,\n",
       "    0.049512892961502075,\n",
       "    0.02615169622004032,\n",
       "    0.047655701637268066,\n",
       "    0.018839722499251366,\n",
       "    0.04651549458503723,\n",
       "    0.01787630282342434,\n",
       "    0.06400342285633087,\n",
       "    -0.0011960380943492055,\n",
       "    -0.03088848665356636,\n",
       "    -0.02353687584400177,\n",
       "    -0.07165618240833282,\n",
       "    -0.004128720611333847,\n",
       "    -0.10213478654623032,\n",
       "    -0.016877423971891403,\n",
       "    0.04141185060143471,\n",
       "    -0.07965841889381409,\n",
       "    -0.015215232037007809,\n",
       "    0.013135495595633984,\n",
       "    0.014800285920500755,\n",
       "    0.04410352185368538,\n",
       "    0.028583252802491188,\n",
       "    0.009225753135979176,\n",
       "    -0.04181870073080063,\n",
       "    -0.08294069766998291,\n",
       "    -0.008384850807487965,\n",
       "    0.03296187147498131,\n",
       "    -0.025631310418248177,\n",
       "    0.022493088617920876,\n",
       "    -0.01674111932516098,\n",
       "    0.050455015152692795,\n",
       "    0.033760812133550644,\n",
       "    0.0690721645951271,\n",
       "    0.08107159286737442,\n",
       "    0.013202343136072159,\n",
       "    -0.052746713161468506,\n",
       "    -0.019190961495041847,\n",
       "    0.033658552914857864,\n",
       "    0.020854393020272255,\n",
       "    0.021034477278590202,\n",
       "    -0.040963396430015564,\n",
       "    0.024653829634189606,\n",
       "    -0.03291517123579979,\n",
       "    -0.004585550166666508,\n",
       "    -0.03231721743941307,\n",
       "    -0.025133516639471054,\n",
       "    -0.017828939482569695,\n",
       "    0.002383673097938299,\n",
       "    -0.008303524926304817,\n",
       "    0.003893317421898246,\n",
       "    -0.015948142856359482,\n",
       "    -0.019910695031285286,\n",
       "    -0.008788744919002056,\n",
       "    -0.030989760532975197,\n",
       "    -0.07181129604578018,\n",
       "    -0.040455013513565063,\n",
       "    0.013978601433336735,\n",
       "    0.019448090344667435,\n",
       "    0.05490962043404579,\n",
       "    0.0126829594373703,\n",
       "    -0.044767577201128006,\n",
       "    0.05884534493088722,\n",
       "    0.036003682762384415,\n",
       "    0.03745252639055252,\n",
       "    0.05568120628595352,\n",
       "    -0.029011858627200127,\n",
       "    -0.002212631981819868,\n",
       "    -0.013629493303596973,\n",
       "    0.029515843838453293,\n",
       "    0.014221939258277416,\n",
       "    0.020167291164398193,\n",
       "    0.00604518735781312,\n",
       "    0.030234649777412415,\n",
       "    0.05822822451591492,\n",
       "    -0.00608811667189002,\n",
       "    0.03667832538485527,\n",
       "    0.013663796707987785,\n",
       "    0.05733579397201538,\n",
       "    -0.004701974801719189,\n",
       "    -0.020501870661973953,\n",
       "    -0.04392627999186516,\n",
       "    0.039572857320308685,\n",
       "    -0.05494236201047897,\n",
       "    -0.045676153153181076,\n",
       "    0.04223782941699028,\n",
       "    -0.012858545407652855,\n",
       "    0.01971423253417015,\n",
       "    -0.023676013574004173,\n",
       "    -0.01556940283626318,\n",
       "    0.022330645471811295,\n",
       "    0.02068895846605301,\n",
       "    -0.01994292438030243,\n",
       "    0.06505676358938217,\n",
       "    -0.012353870086371899,\n",
       "    0.03732313588261604,\n",
       "    0.16658319532871246,\n",
       "    0.09750203788280487,\n",
       "    0.0027498637791723013,\n",
       "    -0.005660920403897762,\n",
       "    -0.011760732159018517,\n",
       "    -0.07513832300901413,\n",
       "    -0.0047700065188109875,\n",
       "    -0.04080720990896225,\n",
       "    0.0015983718913048506,\n",
       "    -0.055550217628479004,\n",
       "    0.011628320440649986,\n",
       "    0.015326674096286297,\n",
       "    0.029573727399110794,\n",
       "    -0.06306710094213486,\n",
       "    -0.03359021246433258,\n",
       "    0.021951686590909958,\n",
       "    -0.034576356410980225,\n",
       "    0.019421521574258804,\n",
       "    0.0375659242272377,\n",
       "    0.007152385078370571,\n",
       "    -0.0041014570742845535,\n",
       "    0.024057578295469284,\n",
       "    -0.04338593780994415,\n",
       "    -0.007564026862382889,\n",
       "    -0.029933545738458633,\n",
       "    -0.006048038136214018,\n",
       "    -0.007228589151054621,\n",
       "    -0.01589215360581875,\n",
       "    0.0045714303851127625,\n",
       "    -0.025394562631845474,\n",
       "    -0.04967103525996208,\n",
       "    -0.04853309318423271,\n",
       "    -0.009038849733769894,\n",
       "    -0.017137212678790092,\n",
       "    0.026564382016658783,\n",
       "    0.003598748706281185,\n",
       "    0.05188576504588127,\n",
       "    -0.04984581470489502,\n",
       "    -0.034325823187828064,\n",
       "    -0.06256308406591415,\n",
       "    0.021465221419930458,\n",
       "    0.013854390941560268,\n",
       "    0.09293340146541595,\n",
       "    -0.030491245910525322,\n",
       "    -0.0025441681500524282,\n",
       "    0.025187792256474495,\n",
       "    0.03488205745816231,\n",
       "    0.006566612981259823,\n",
       "    0.006832854822278023,\n",
       "    -0.011919518932700157,\n",
       "    -0.015182492323219776,\n",
       "    -0.0011526839807629585,\n",
       "    0.07332991063594818,\n",
       "    -0.037710364907979965,\n",
       "    0.03769277036190033,\n",
       "    0.026313139125704765,\n",
       "    -0.04767938330769539,\n",
       "    -0.03616640716791153,\n",
       "    0.02400086633861065,\n",
       "    0.017886918038129807,\n",
       "    -0.05663930997252464,\n",
       "    -0.02063990943133831,\n",
       "    0.026371285319328308,\n",
       "    -0.016565950587391853,\n",
       "    0.04762883484363556,\n",
       "    -0.05932890996336937,\n",
       "    0.04281012341380119,\n",
       "    0.003590889973565936,\n",
       "    -0.02726716734468937,\n",
       "    -0.06154657155275345,\n",
       "    -0.011628026142716408,\n",
       "    0.03170665726065636,\n",
       "    -0.0015739093068987131,\n",
       "    -0.03807350993156433,\n",
       "    0.04260468855500221,\n",
       "    0.014341422356665134,\n",
       "    -0.034845709800720215,\n",
       "    -0.004653605166822672,\n",
       "    -0.011251605115830898,\n",
       "    0.01304748933762312,\n",
       "    -0.02040032111108303,\n",
       "    -0.03085586428642273,\n",
       "    0.00833528209477663,\n",
       "    -0.07130128145217896,\n",
       "    -0.01006266288459301,\n",
       "    0.03073619119822979,\n",
       "    0.004811029881238937,\n",
       "    0.023529158905148506,\n",
       "    0.03680652007460594,\n",
       "    -0.016641955822706223,\n",
       "    -0.03500646352767944,\n",
       "    -0.011404486373066902,\n",
       "    0.04458816349506378,\n",
       "    -0.025068067014217377,\n",
       "    0.022336464375257492,\n",
       "    0.028296805918216705,\n",
       "    -0.012704658322036266,\n",
       "    -0.02440457046031952,\n",
       "    0.004808304365724325,\n",
       "    -0.00956078339368105,\n",
       "    -0.010184313170611858,\n",
       "    -0.022572411224246025,\n",
       "    -0.013667943887412548,\n",
       "    0.02824634313583374,\n",
       "    0.012692433781921864,\n",
       "    0.028408264741301537,\n",
       "    -0.00204469938762486,\n",
       "    0.02014605700969696,\n",
       "    -0.004876263905316591,\n",
       "    0.0753818228840828,\n",
       "    -0.041747044771909714,\n",
       "    -0.0258479006588459,\n",
       "    0.042281366884708405,\n",
       "    0.035405635833740234,\n",
       "    -0.0290506724268198,\n",
       "    0.04593971371650696,\n",
       "    0.04125683754682541,\n",
       "    0.013610896654427052,\n",
       "    0.025132643058896065,\n",
       "    -0.01849384792149067,\n",
       "    0.007784785702824593,\n",
       "    0.024523120373487473,\n",
       "    -0.0001152896074927412,\n",
       "    -0.02222384139895439,\n",
       "    0.020832503214478493,\n",
       "    -0.0103781558573246,\n",
       "    -0.01979125663638115,\n",
       "    0.023134291172027588,\n",
       "    -0.047275085002183914,\n",
       "    0.015459885820746422,\n",
       "    -0.016358517110347748,\n",
       "    0.0035562203265726566,\n",
       "    0.016856519505381584,\n",
       "    -0.013399968855082989,\n",
       "    0.004254445433616638,\n",
       "    -0.018420355394482613,\n",
       "    0.018162161111831665,\n",
       "    0.019625095650553703,\n",
       "    0.04486647993326187,\n",
       "    -0.005344994366168976,\n",
       "    0.022518040612339973,\n",
       "    -0.039562907069921494,\n",
       "    -0.021284162998199463,\n",
       "    0.01225263625383377,\n",
       "    0.13282552361488342,\n",
       "    -0.005380573216825724,\n",
       "    0.01330203004181385,\n",
       "    0.006166552193462849,\n",
       "    -0.021860850974917412,\n",
       "    0.003640142735093832,\n",
       "    -0.0006246823468245566,\n",
       "    -0.034894220530986786,\n",
       "    -0.03779197856783867,\n",
       "    -0.00945043470710516,\n",
       "    -0.06820233166217804,\n",
       "    -0.02119751274585724,\n",
       "    -0.010652792640030384,\n",
       "    0.042757775634527206,\n",
       "    -0.007720456458628178,\n",
       "    -0.014772269874811172,\n",
       "    -0.07037943601608276,\n",
       "    0.013158022426068783,\n",
       "    -0.003492549993097782,\n",
       "    0.003895081114023924,\n",
       "    0.05458278954029083,\n",
       "    0.027309052646160126,\n",
       "    -0.008095766417682171,\n",
       "    -0.0008344621164724231,\n",
       "    0.0395088717341423,\n",
       "    -0.028880085796117783,\n",
       "    -0.008751519955694675,\n",
       "    -0.002900552935898304,\n",
       "    -0.0022632486652582884,\n",
       "    -0.03499261289834976,\n",
       "    0.04126131534576416,\n",
       "    -0.027918493375182152,\n",
       "    -0.046213265508413315,\n",
       "    0.00706042954698205,\n",
       "    -0.008781560696661472,\n",
       "    -0.035093754529953,\n",
       "    0.015777967870235443,\n",
       "    -0.012421578168869019,\n",
       "    0.01928001455962658,\n",
       "    0.032770752906799316,\n",
       "    0.028050143271684647,\n",
       "    0.025669699534773827,\n",
       "    -0.007092077750712633,\n",
       "    -0.01248664315789938,\n",
       "    0.020954133942723274,\n",
       "    -0.01563405431807041,\n",
       "    0.004348660819232464,\n",
       "    -0.010191662237048149,\n",
       "    0.02109442465007305,\n",
       "    0.0028238457161933184,\n",
       "    -0.010689863935112953,\n",
       "    -0.048729099333286285,\n",
       "    -0.06669989973306656,\n",
       "    0.002748158061876893,\n",
       "    0.0017983689904212952,\n",
       "    -0.018574479967355728,\n",
       "    -0.014314897358417511,\n",
       "    -0.011860797181725502,\n",
       "    0.003567651379853487,\n",
       "    -0.0072482675313949585,\n",
       "    -0.008826293051242828,\n",
       "    0.010035617277026176,\n",
       "    0.02034858427941799,\n",
       "    0.01246492750942707,\n",
       "    0.009398817084729671,\n",
       "    0.015827877447009087,\n",
       "    0.02133425511419773,\n",
       "    0.06164707988500595,\n",
       "    0.0068054948933422565,\n",
       "    0.006929254159331322,\n",
       "    0.02327793836593628,\n",
       "    0.015519517473876476,\n",
       "    0.0007811565883457661,\n",
       "    0.004319335799664259,\n",
       "    -0.010352369397878647,\n",
       "    -0.00570618687197566,\n",
       "    -0.03876446187496185,\n",
       "    -0.007051181513816118,\n",
       "    0.00841574463993311,\n",
       "    -0.0042666844092309475,\n",
       "    0.03968470171093941,\n",
       "    -0.018240364268422127,\n",
       "    0.02861073613166809,\n",
       "    -0.04869457706809044,\n",
       "    0.003310665488243103,\n",
       "    0.009084515273571014,\n",
       "    -0.004054335877299309,\n",
       "    -0.007540013641119003,\n",
       "    0.057182420045137405,\n",
       "    0.07313603162765503,\n",
       "    -0.07344289124011993,\n",
       "    -0.003542074002325535,\n",
       "    -0.005301071796566248,\n",
       "    0.05926268920302391,\n",
       "    -0.009247532114386559,\n",
       "    0.0007646703743375838,\n",
       "    0.029464032500982285,\n",
       "    -0.0065568541176617146,\n",
       "    0.030366932973265648,\n",
       "    -0.019454125314950943,\n",
       "    0.012774010188877583,\n",
       "    0.009846468456089497,\n",
       "    0.013020259328186512,\n",
       "    0.038997307419776917,\n",
       "    0.01008826307952404,\n",
       "    -0.03506874293088913,\n",
       "    0.010852869600057602,\n",
       "    -0.02627788670361042,\n",
       "    -0.007105296477675438,\n",
       "    0.03382740914821625,\n",
       "    0.04961186274886131,\n",
       "    -0.0495404414832592,\n",
       "    -0.020383182913064957,\n",
       "    -0.024995580315589905,\n",
       "    0.010430210269987583,\n",
       "    -0.036666713654994965,\n",
       "    -0.03917868062853813,\n",
       "    -0.00659341923892498,\n",
       "    -0.0014735461445525289,\n",
       "    0.005631183739751577,\n",
       "    0.016437049955129623,\n",
       "    -0.005933514796197414,\n",
       "    0.009484532289206982,\n",
       "    0.006670215167105198,\n",
       "    -0.014449224807322025,\n",
       "    0.04060032591223717,\n",
       "    0.01709972880780697,\n",
       "    -0.0049807727336883545,\n",
       "    0.03749261051416397,\n",
       "    0.016734004020690918,\n",
       "    -0.008831783197820187,\n",
       "    0.025320734828710556,\n",
       "    0.006579867098480463,\n",
       "    0.059233177453279495,\n",
       "    -0.005480610765516758,\n",
       "    0.010545832104980946,\n",
       "    -0.00502040283754468,\n",
       "    -0.03523178771138191,\n",
       "    0.018012262880802155,\n",
       "    0.006397150456905365,\n",
       "    0.05526193231344223,\n",
       "    -0.019518425688147545,\n",
       "    -0.01626235991716385,\n",
       "    -0.012914045713841915,\n",
       "    0.012677101418375969,\n",
       "    0.034318745136260986,\n",
       "    0.0643266960978508,\n",
       "    0.0006262310198508203,\n",
       "    0.027457769960165024,\n",
       "    0.004468724597245455,\n",
       "    0.03412662446498871,\n",
       "    -0.03832077980041504,\n",
       "    0.01519427914172411,\n",
       "    0.011779801920056343,\n",
       "    0.02536710537970066,\n",
       "    -0.020955227315425873,\n",
       "    -0.004851075354963541,\n",
       "    0.0471133217215538,\n",
       "    -0.04194381833076477,\n",
       "    -0.019742842763662338,\n",
       "    0.03880087658762932,\n",
       "    -0.06363067030906677,\n",
       "    -0.04640374332666397,\n",
       "    -0.032905153930187225,\n",
       "    0.017684707418084145,\n",
       "    0.002736323280259967,\n",
       "    0.03078353777527809,\n",
       "    0.010877287946641445,\n",
       "    0.04473946988582611,\n",
       "    -0.005496627651154995,\n",
       "    0.033924657851457596,\n",
       "    0.03890001028776169,\n",
       "    0.012095184996724129,\n",
       "    0.005891533102840185,\n",
       "    -0.01316206343472004,\n",
       "    0.0035454367753118277,\n",
       "    8.341106149600819e-05,\n",
       "    -0.03794345259666443,\n",
       "    -0.017890632152557373,\n",
       "    0.04628283903002739,\n",
       "    0.008938726037740707,\n",
       "    -0.0031263085547834635,\n",
       "    -0.002857136307284236,\n",
       "    0.03500746190547943,\n",
       "    -0.03723277151584625,\n",
       "    0.02206476777791977,\n",
       "    -0.0058040074072778225,\n",
       "    -0.02430245652794838,\n",
       "    -0.031813059002161026,\n",
       "    -0.02646479196846485,\n",
       "    0.030746817588806152,\n",
       "    -0.0009103936608880758,\n",
       "    0.01610488072037697,\n",
       "    -0.012209687381982803,\n",
       "    0.015752043575048447,\n",
       "    -0.026676051318645477,\n",
       "    0.011242823675274849,\n",
       "    0.03546246141195297,\n",
       "    -0.02519456297159195,\n",
       "    -0.026363370940089226,\n",
       "    0.03527523949742317,\n",
       "    0.013071984052658081,\n",
       "    0.009163354523479939,\n",
       "    0.0023725242353975773,\n",
       "    0.0046413615345954895,\n",
       "    -0.05054108798503876,\n",
       "    0.010714872740209103,\n",
       "    0.017129836603999138,\n",
       "    0.007905674166977406,\n",
       "    -0.03371873497962952,\n",
       "    0.01606220006942749,\n",
       "    -0.0006231364095583558,\n",
       "    0.02545985020697117,\n",
       "    0.005366732832044363,\n",
       "    -0.01836448349058628,\n",
       "    -0.011988461017608643,\n",
       "    -0.019413726404309273,\n",
       "    0.015414279885590076,\n",
       "    0.02980228327214718,\n",
       "    -0.02348509058356285,\n",
       "    0.003193808952346444,\n",
       "    -0.052208878099918365,\n",
       "    -0.03297707810997963,\n",
       "    -0.04378284513950348,\n",
       "    0.0022447523660957813,\n",
       "    0.011969195678830147,\n",
       "    -0.04242713004350662,\n",
       "    0.011374136433005333,\n",
       "    0.026855837553739548,\n",
       "    0.03855689987540245,\n",
       "    0.007108194287866354,\n",
       "    0.007358178496360779,\n",
       "    -0.020325222983956337,\n",
       "    0.006258615292608738,\n",
       "    0.03332296758890152,\n",
       "    0.030697215348482132,\n",
       "    0.07619540393352509,\n",
       "    -0.003405374940484762,\n",
       "    -0.010322604328393936,\n",
       "    -0.031194431707262993,\n",
       "    -0.0284273661673069,\n",
       "    -0.012737405486404896,\n",
       "    -0.027485018596053123,\n",
       "    0.04743136093020439,\n",
       "    0.008246547542512417,\n",
       "    0.0012554270215332508,\n",
       "    0.003295905888080597,\n",
       "    -0.0021393410861492157,\n",
       "    -0.0700768530368805,\n",
       "    0.013290142640471458,\n",
       "    0.02161893993616104,\n",
       "    -0.0005259562749415636,\n",
       "    -0.021908428519964218,\n",
       "    0.04808409512042999,\n",
       "    -0.0073266117833554745,\n",
       "    0.04366520419716835,\n",
       "    -0.009438113309442997,\n",
       "    -0.011312160640954971,\n",
       "    0.03650369867682457,\n",
       "    0.029848327860236168,\n",
       "    -0.016708077862858772,\n",
       "    0.019851546734571457,\n",
       "    0.008947165682911873,\n",
       "    0.0065270671620965,\n",
       "    0.015336643904447556,\n",
       "    -0.01689053513109684,\n",
       "    -0.023741206154227257,\n",
       "    -0.03290988504886627,\n",
       "    0.04883761703968048,\n",
       "    -0.03386174142360687,\n",
       "    -0.005740444641560316,\n",
       "    0.01516628172248602,\n",
       "    -0.0414544939994812,\n",
       "    -0.00869925506412983,\n",
       "    -0.024811681360006332,\n",
       "    0.027583783492445946,\n",
       "    -0.012378295883536339,\n",
       "    0.013350049965083599,\n",
       "    0.004624347668141127,\n",
       "    -0.027783410623669624,\n",
       "    -0.0437723733484745,\n",
       "    -0.008707297034561634,\n",
       "    -0.002139851450920105,\n",
       "    0.02196197211742401,\n",
       "    -0.003743086475878954,\n",
       "    0.023401612415909767,\n",
       "    -0.035060230642557144,\n",
       "    0.03601996228098869,\n",
       "    -0.0017041063401848078,\n",
       "    0.012490035966038704,\n",
       "    -0.0004412209091242403,\n",
       "    0.0220374446362257,\n",
       "    -0.009576474316418171,\n",
       "    -0.039425354450941086,\n",
       "    0.02187667414546013,\n",
       "    -0.03784174472093582,\n",
       "    0.01898478902876377,\n",
       "    -0.009587029926478863,\n",
       "    0.017371386289596558,\n",
       "    -0.022606775164604187,\n",
       "    -0.00646258145570755,\n",
       "    0.04503777250647545,\n",
       "    -0.008563904091715813,\n",
       "    -0.053409893065690994,\n",
       "    0.015968305990099907,\n",
       "    -0.035688742995262146,\n",
       "    -0.0010443180799484253,\n",
       "    0.013026232831180096,\n",
       "    -0.04161887243390083,\n",
       "    -0.03353658318519592,\n",
       "    0.037145692855119705,\n",
       "    0.021408235654234886,\n",
       "    0.0185339767485857,\n",
       "    -0.0382472462952137,\n",
       "    -0.018955089151859283,\n",
       "    -0.0023882254026830196,\n",
       "    0.001439841347746551,\n",
       "    0.022817516699433327,\n",
       "    0.007928824052214622,\n",
       "    -0.00780259445309639,\n",
       "    0.01915597915649414,\n",
       "    -0.018029317259788513,\n",
       "    0.023088909685611725,\n",
       "    -0.006085866596549749,\n",
       "    0.01538693904876709,\n",
       "    -0.038643598556518555,\n",
       "    -0.00599348358809948,\n",
       "    0.019129550084471703,\n",
       "    0.021311501041054726,\n",
       "    -0.017255118116736412,\n",
       "    -0.0004319218569435179,\n",
       "    -0.05994977056980133,\n",
       "    -0.03862766548991203,\n",
       "    0.013892949558794498,\n",
       "    0.01801205426454544,\n",
       "    -0.03409777209162712,\n",
       "    -0.0021542960312217474,\n",
       "    0.03125063702464104,\n",
       "    0.008625179529190063,\n",
       "    -0.018837517127394676,\n",
       "    -0.009458177722990513,\n",
       "    -0.03305863216519356,\n",
       "    0.0011124234879389405,\n",
       "    0.009412667714059353,\n",
       "    0.020790083333849907,\n",
       "    -0.019670680165290833,\n",
       "    -0.011790861375629902,\n",
       "    -0.07488175481557846,\n",
       "    0.029666215181350708,\n",
       "    0.03188477084040642,\n",
       "    -0.011736695654690266,\n",
       "    -0.014789336360991001,\n",
       "    0.016080589964985847,\n",
       "    -0.005476002581417561,\n",
       "    -0.0013252259232103825,\n",
       "    -0.02810887061059475,\n",
       "    0.0029783493373543024,\n",
       "    0.026508407667279243,\n",
       "    -0.004841184243559837,\n",
       "    0.00829470157623291,\n",
       "    0.0371875986456871,\n",
       "    0.040942296385765076,\n",
       "    -0.09342274814844131,\n",
       "    0.019696075469255447,\n",
       "    0.013794085010886192,\n",
       "    -0.007359077222645283,\n",
       "    0.016296416521072388,\n",
       "    -0.01960671879351139,\n",
       "    -0.020376035943627357,\n",
       "    -0.00026122256531380117,\n",
       "    0.046291887760162354,\n",
       "    0.01133748609572649,\n",
       "    0.04442944750189781,\n",
       "    -0.01898842118680477,\n",
       "    0.01369770523160696,\n",
       "    0.08547674864530563,\n",
       "    -0.0237030740827322,\n",
       "    -0.004855619743466377,\n",
       "    -0.05040588229894638,\n",
       "    0.004656523000448942,\n",
       "    0.025163501501083374,\n",
       "    -0.008057494647800922,\n",
       "    -0.031595636159181595,\n",
       "    0.001346113858744502,\n",
       "    0.0015921816229820251,\n",
       "    -0.003169612493366003,\n",
       "    0.031206615269184113,\n",
       "    0.023938661441206932,\n",
       "    -0.036655161529779434,\n",
       "    -0.009145560674369335,\n",
       "    -0.014612600207328796,\n",
       "    0.033448804169893265,\n",
       "    -0.005648559890687466,\n",
       "    -0.007854143157601357,\n",
       "    -0.02229953557252884,\n",
       "    -0.02276640012860298,\n",
       "    -0.042613644152879715,\n",
       "    0.014064356684684753,\n",
       "    0.0023513715714216232,\n",
       "    -0.013175862841308117,\n",
       "    -0.057138219475746155,\n",
       "    0.006743876729160547,\n",
       "    -0.04313810169696808,\n",
       "    0.04597974941134453,\n",
       "    -0.02173168770968914,\n",
       "    -0.03402217850089073,\n",
       "    -0.020175635814666748,\n",
       "    -0.013330098241567612,\n",
       "    0.04082190990447998,\n",
       "    0.0176689550280571,\n",
       "    -0.02657548151910305,\n",
       "    0.0006731588509865105,\n",
       "    0.016198785975575447,\n",
       "    0.006230911705642939,\n",
       "    -0.01092398539185524,\n",
       "    0.030514219775795937,\n",
       "    -0.027502957731485367,\n",
       "    0.020729320123791695,\n",
       "    -0.004886279813945293,\n",
       "    -0.00012835308734793216,\n",
       "    0.02727951854467392,\n",
       "    0.0010984605178236961,\n",
       "    0.04741119220852852,\n",
       "    0.04192299768328667,\n",
       "    0.046636682003736496,\n",
       "    -0.010088629089295864,\n",
       "    0.009014513343572617,\n",
       "    -0.001582006923854351,\n",
       "    0.028425028547644615,\n",
       "    -0.024896668270230293,\n",
       "    -0.004552243743091822,\n",
       "    0.028544947504997253,\n",
       "    -0.008262756280601025,\n",
       "    0.00025387664209119976,\n",
       "    0.010415351949632168,\n",
       "    -0.016652341932058334,\n",
       "    0.012241996824741364,\n",
       "    0.013956957496702671,\n",
       "    0.037928204983472824,\n",
       "    0.03070381097495556,\n",
       "    -0.01615995354950428,\n",
       "    -0.016415610909461975,\n",
       "    0.04673212021589279,\n",
       "    -0.0015661261277273297,\n",
       "    -0.022142639383673668,\n",
       "    -0.04879032075405121,\n",
       "    -0.017964458093047142,\n",
       "    0.007384466007351875,\n",
       "    -0.03771410137414932,\n",
       "    0.03195902705192566,\n",
       "    -0.01427142508327961,\n",
       "    -0.0077812327072024345,\n",
       "    -0.004425975494086742,\n",
       "    0.011814874596893787,\n",
       "    0.008658005855977535,\n",
       "    -0.00204340647906065,\n",
       "    -0.024599002674221992,\n",
       "    -9.380847041029483e-05,\n",
       "    -0.013314523734152317,\n",
       "    -0.058482393622398376,\n",
       "    0.019023168832063675,\n",
       "    -0.020859280601143837,\n",
       "    0.03921281173825264,\n",
       "    -0.005980777554214001,\n",
       "    -0.027253713458776474,\n",
       "    0.023058190941810608,\n",
       "    0.04563206806778908,\n",
       "    -0.0349467396736145,\n",
       "    -0.02453865110874176,\n",
       "    -0.03955705836415291,\n",
       "    0.01934358850121498,\n",
       "    0.004840594716370106,\n",
       "    -0.06062662973999977,\n",
       "    0.0031433345284312963,\n",
       "    -0.029099876061081886,\n",
       "    -0.01702103391289711,\n",
       "    0.00431734649464488,\n",
       "    -0.02286738157272339,\n",
       "    -0.04511198401451111,\n",
       "    -0.04892852157354355,\n",
       "    -0.01986737921833992,\n",
       "    0.053627170622348785,\n",
       "    -0.028527043759822845,\n",
       "    0.058330778032541275,\n",
       "    -0.021419517695903778,\n",
       "    0.035209693014621735,\n",
       "    -0.008945184759795666,\n",
       "    0.014856650494039059,\n",
       "    0.01907576061785221,\n",
       "    -0.006358148530125618,\n",
       "    0.019940325990319252,\n",
       "    -0.01093036588281393,\n",
       "    -0.028423532843589783,\n",
       "    0.005733700934797525,\n",
       "    0.010363823734223843,\n",
       "    -0.012528926134109497,\n",
       "    -0.007765110116451979,\n",
       "    -0.00888136588037014,\n",
       "    -0.02176506258547306,\n",
       "    -0.014181874692440033,\n",
       "    0.0002571552467998117,\n",
       "    -0.01008828915655613,\n",
       "    0.0021472868975251913,\n",
       "    -0.04989485815167427,\n",
       "    -0.03014678880572319,\n",
       "    -0.03285830095410347,\n",
       "    0.013427508063614368,\n",
       "    0.020972954109311104,\n",
       "    -0.040666066110134125,\n",
       "    -0.05945175886154175,\n",
       "    -0.0407930426299572,\n",
       "    -0.025721319019794464,\n",
       "    -0.057627469301223755,\n",
       "    -0.011538433842360973,\n",
       "    -0.04040399566292763,\n",
       "    0.006018467713147402,\n",
       "    -0.008339062333106995,\n",
       "    -0.0218377523124218,\n",
       "    0.01911812275648117,\n",
       "    -0.030190952122211456,\n",
       "    0.012116871774196625,\n",
       "    -0.009978296235203743,\n",
       "    0.03206821531057358,\n",
       "    -0.02509908378124237,\n",
       "    -0.025295449420809746,\n",
       "    -0.03243381902575493,\n",
       "    -0.016597801819443703,\n",
       "    0.00218791700899601,\n",
       "    0.013982133939862251,\n",
       "    -0.00913823489099741,\n",
       "    0.023627232760190964,\n",
       "    0.0143702058121562,\n",
       "    0.0014168855268508196,\n",
       "    0.013057262636721134,\n",
       "    0.05640557035803795,\n",
       "    -0.022262554615736008,\n",
       "    0.036393605172634125,\n",
       "    -0.026596674695611,\n",
       "    0.0023987365420907736,\n",
       "    -0.032315030694007874,\n",
       "    0.0022877694573253393,\n",
       "    -0.029907314106822014,\n",
       "    0.0040907710790634155,\n",
       "    0.01701897196471691,\n",
       "    -0.03993702679872513,\n",
       "    -0.027992691844701767,\n",
       "    -0.007560213562101126,\n",
       "    -0.00012573723506648093,\n",
       "    -0.025707805529236794,\n",
       "    -0.059072863310575485,\n",
       "    0.006452465429902077,\n",
       "    0.00826883502304554,\n",
       "    -0.03275799751281738,\n",
       "    -0.023842329159379005,\n",
       "    -0.0037800613790750504,\n",
       "    0.007571636699140072,\n",
       "    0.05230419710278511,\n",
       "    -0.04990610480308533,\n",
       "    -0.015810750424861908,\n",
       "    0.009472993202507496,\n",
       "    0.020366016775369644,\n",
       "    0.004400002304464579,\n",
       "    0.010122603736817837,\n",
       "    -0.03267688676714897,\n",
       "    0.031137466430664062,\n",
       "    -0.0007322984747588634,\n",
       "    0.018381178379058838,\n",
       "    0.015022613108158112,\n",
       "    0.021970456466078758,\n",
       "    0.02781263180077076,\n",
       "    0.011297213844954967,\n",
       "    -0.000965763465501368,\n",
       "    -0.009082810021936893,\n",
       "    0.01595172844827175,\n",
       "    0.011029507033526897,\n",
       "    0.030590875074267387,\n",
       "    -0.013937566429376602,\n",
       "    0.02559548057615757,\n",
       "    0.015535353682935238,\n",
       "    0.04243415594100952,\n",
       "    0.019220558926463127,\n",
       "    0.015859592705965042,\n",
       "    0.027663834393024445,\n",
       "    0.029194993898272514,\n",
       "    0.004013379104435444,\n",
       "    0.020621197298169136,\n",
       "    0.007484243251383305,\n",
       "    -0.03746578097343445,\n",
       "    -0.005498996935784817,\n",
       "    -0.020581038668751717,\n",
       "    0.02295074611902237,\n",
       "    0.001210200716741383,\n",
       "    -0.024350976571440697,\n",
       "    0.03534698858857155,\n",
       "    0.02306641824543476,\n",
       "    0.003747324924916029,\n",
       "    0.005862789694219828,\n",
       "    0.021780122071504593,\n",
       "    0.013052957132458687,\n",
       "    0.0012886603362858295,\n",
       "    0.0031216773204505444,\n",
       "    -0.014485138468444347,\n",
       "    0.014252496883273125,\n",
       "    0.018555711954832077,\n",
       "    -0.004851235542446375,\n",
       "    -0.04045432433485985,\n",
       "    0.061197683215141296,\n",
       "    0.008314167149364948,\n",
       "    -0.028856271877884865,\n",
       "    -0.05436746031045914,\n",
       "    -0.024497851729393005,\n",
       "    0.03482611104846001,\n",
       "    0.05962910130620003,\n",
       "    0.016818219795823097,\n",
       "    -0.0026782790664583445,\n",
       "    -0.015114642679691315,\n",
       "    0.019394181668758392,\n",
       "    0.017556391656398773,\n",
       "    0.02047964744269848,\n",
       "    -0.018174536526203156,\n",
       "    0.04991519823670387,\n",
       "    -0.014015414752066135,\n",
       "    -0.010001149959862232,\n",
       "    -0.03958456218242645,\n",
       "    0.02197917550802231,\n",
       "    0.004995292518287897,\n",
       "    -0.010035618208348751,\n",
       "    0.00742489704862237,\n",
       "    -0.019060082733631134,\n",
       "    0.01042209379374981,\n",
       "    -0.03204863891005516,\n",
       "    0.016638509929180145,\n",
       "    2.4213146389229223e-05,\n",
       "    -0.049743954092264175,\n",
       "    -0.014334344305098057,\n",
       "    0.021350491791963577,\n",
       "    -0.0001825967920012772,\n",
       "    0.02992023341357708,\n",
       "    -0.004887828603386879,\n",
       "    0.0212637260556221,\n",
       "    -0.028907785192131996,\n",
       "    0.014538767747581005,\n",
       "    0.04368671402335167,\n",
       "    0.04843699932098389,\n",
       "    -0.006561088375747204,\n",
       "    -0.037719760090112686,\n",
       "    0.0030284984968602657,\n",
       "    0.029481682926416397,\n",
       "    0.03010151907801628,\n",
       "    0.046011749655008316,\n",
       "    -0.03111637942492962,\n",
       "    -0.008755531162023544,\n",
       "    0.039334896951913834,\n",
       "    0.05325258895754814,\n",
       "    -0.016084158793091774,\n",
       "    0.04816783592104912,\n",
       "    0.03705954551696777,\n",
       "    -0.01562522165477276,\n",
       "    0.00254835095256567,\n",
       "    -0.015629751607775688,\n",
       "    0.0014638389693573117,\n",
       "    -0.03840199485421181,\n",
       "    0.004763689823448658,\n",
       "    0.007668601348996162,\n",
       "    -0.042919497936964035,\n",
       "    0.013733813539147377,\n",
       "    -0.013956887647509575,\n",
       "    -0.027113167569041252,\n",
       "    0.021589701995253563,\n",
       "    -0.03492270037531853,\n",
       "    0.009915471076965332,\n",
       "    -0.004942362662404776,\n",
       "    0.007079944480210543,\n",
       "    0.021038014441728592,\n",
       "    0.032308392226696014,\n",
       "    -0.00792398490011692,\n",
       "    0.024706095457077026,\n",
       "    0.001986371586099267,\n",
       "    -0.012682761065661907,\n",
       "    -0.0021897293627262115,\n",
       "    0.03445090726017952,\n",
       "    ...],\n",
       "   'rag_used': True,\n",
       "   'history_used': True,\n",
       "   'rag_and_history_used': True,\n",
       "   'bm25_used': True,\n",
       "   'topic_retrieval_used': True,\n",
       "   'desired_lookback_window_size': 3,\n",
       "   'actual_lookback_window_size': 0,\n",
       "   'desired_top_k_chunks': 5,\n",
       "   'query_start_time': '2025-05-01T15:51:07.422516',\n",
       "   'query_finish_time': '2025-05-01T15:51:19.691104',\n",
       "   'query_processing_time': 12.268574476242065,\n",
       "   'context_ids_utilized': [7153, 7159, 7161, 7160, 7152],\n",
       "   'context_ids_source': ['hybrid_topic',\n",
       "    'hybrid_topic',\n",
       "    'hybrid_topic',\n",
       "    'hybrid_topic',\n",
       "    'hybrid_topic'],\n",
       "   'response_text': \"I am doing well, thank you for asking! How can I assist you today?\\n\\nBased on the provided context, it seems that the AI has access to conversational models and can engage in dialogues.\\nIf you have any questions or topics you'd like to discuss, please feel free to ask!\\n\",\n",
       "   'prompt_token_count': 3969,\n",
       "   'candidates_token_count': 62,\n",
       "   'total_token_count': 4031,\n",
       "   'system_prompt_used': \"\\n    You are an advanced AI research assistant. Generate detailed and comprehensive responses that supplement students' and academic researchers' work with information grounded in highly cited AI/ML research papers, specifically in fields like NLP and CV. The response should not focus on one area of study but should be informed by both the current query and chat history to generate a well-rounded answer.\\n\\n    1. **Introductory Overview**: Start with a high-level conceptual overview of the topic, providing a brief and clear explanation that covers the essential aspects of the subject. This should be accessible to a broad audience.\\n\\n    2. **Technical Overview**: After the conceptual overview, provide a more in-depth, technical explanation that dives deeper into the topic. This could include relevant algorithms, methods, or models, as well as their theoretical foundations.\\n\\n    3. **Example-Based Expansion**: Throughout the response, incorporate examples from relevant research to illustrate key concepts. These examples should come from generalized research trends and not focus on specific papers or studies, helping to broaden the context.\\n\\n    4. **Broader Exploration**: After addressing the original query, provide suggestions for related topics or areas for further exploration, encouraging the user to expand their understanding. The exploration should relate to the current query and prior query/response pairs, offering natural extensions to the discussion, such as other approaches, applications, or advancements related to the topic.\\n\\n    The tone should be professional yet approachable, offering a balance of conceptual clarity and technical depth. The response should not be overly simplistic, but should aim to make complex topics understandable while offering substantial detail. Use direct quotes where relevant, but focus primarily on summarizing findings from academic research.\\n    \",\n",
       "   'dynamic_prompt_body': '\\nCURRENT QUERY 1: hello, how are you today?',\n",
       "   'current_state_context_ids': [6661,\n",
       "    2261,\n",
       "    4087,\n",
       "    2264,\n",
       "    4109,\n",
       "    2263,\n",
       "    4160,\n",
       "    10362,\n",
       "    6660,\n",
       "    4096,\n",
       "    4098,\n",
       "    5646,\n",
       "    2154,\n",
       "    4081,\n",
       "    4162,\n",
       "    9000,\n",
       "    2265,\n",
       "    4306,\n",
       "    4676,\n",
       "    1318,\n",
       "    199,\n",
       "    6543,\n",
       "    7608,\n",
       "    1631,\n",
       "    4427,\n",
       "    2262,\n",
       "    8673,\n",
       "    565,\n",
       "    8091,\n",
       "    1635,\n",
       "    4161,\n",
       "    6334,\n",
       "    4088,\n",
       "    7153,\n",
       "    4809,\n",
       "    6718,\n",
       "    8181,\n",
       "    4810,\n",
       "    4674,\n",
       "    5010,\n",
       "    455,\n",
       "    7609,\n",
       "    873,\n",
       "    1200,\n",
       "    4813,\n",
       "    7516,\n",
       "    333,\n",
       "    4515,\n",
       "    1640,\n",
       "    4071],\n",
       "   'current_state_context_scores': [0.41885296815389833,\n",
       "    0.3946566616061574,\n",
       "    0.387940964020582,\n",
       "    0.37720482680268613,\n",
       "    0.36908266508621745,\n",
       "    0.36906637085948224,\n",
       "    0.3683496479671902,\n",
       "    0.36376999271442145,\n",
       "    0.36368840653140555,\n",
       "    0.3572730930983209,\n",
       "    0.3459066129999425,\n",
       "    0.34065876275849316,\n",
       "    0.3396597102638153,\n",
       "    0.33965711155331624,\n",
       "    0.33939978268340365,\n",
       "    0.3373950516711549,\n",
       "    0.3371025196473253,\n",
       "    0.33439020957766863,\n",
       "    0.3296665308001444,\n",
       "    0.3272784317638613,\n",
       "    0.32706191208801616,\n",
       "    0.3229214324030906,\n",
       "    0.3217469939228273,\n",
       "    0.3216911324022066,\n",
       "    0.3203824639442069,\n",
       "    0.3183960889851306,\n",
       "    0.31626419007039297,\n",
       "    0.3151582016096969,\n",
       "    0.3142976905701607,\n",
       "    0.3106047733423297,\n",
       "    0.30814046689856045,\n",
       "    0.307688009338586,\n",
       "    0.3070195882143729,\n",
       "    0.3064414230018782,\n",
       "    0.30632548088889044,\n",
       "    0.30594175206578833,\n",
       "    0.30477515163612656,\n",
       "    0.30358558845568717,\n",
       "    0.3008147713141126,\n",
       "    0.3001898958742617,\n",
       "    0.29930911283758743,\n",
       "    0.2981276157670625,\n",
       "    0.2969903335541433,\n",
       "    0.2967303939180118,\n",
       "    0.2964125816751396,\n",
       "    0.2959066647856409,\n",
       "    0.2948957446501802,\n",
       "    0.2918277173776195,\n",
       "    0.2913970763528626,\n",
       "    0.29117016334927454],\n",
       "   'retrieval_top_k': 50,\n",
       "   'retrieval_method': 'semantic',\n",
       "   'avg_similarity_to_context': 0.32666429463706664,\n",
       "   'max_similarity_to_context': 0.41885296815389833,\n",
       "   'top_context_id': 6661,\n",
       "   'top_context_score': 0.41885296815389833,\n",
       "   'considered_prior_state_ids': None,\n",
       "   'utilized_prior_state_ids': None,\n",
       "   'filter_similarity_score_threshold': 0.3,\n",
       "   'filter_similarity_score_mask': None,\n",
       "   'bm25_RRF_constant': 60,\n",
       "   'bm25_multiplier': 10,\n",
       "   'current_state_bm25_context_ids': [4162,\n",
       "    2262,\n",
       "    4160,\n",
       "    7168,\n",
       "    7162,\n",
       "    7609,\n",
       "    1637,\n",
       "    7161,\n",
       "    2260,\n",
       "    2261,\n",
       "    7153,\n",
       "    7562,\n",
       "    1640,\n",
       "    4425,\n",
       "    4192,\n",
       "    4893,\n",
       "    1649,\n",
       "    2266,\n",
       "    4426,\n",
       "    4892,\n",
       "    4161,\n",
       "    1632,\n",
       "    7156,\n",
       "    10305,\n",
       "    7558,\n",
       "    2264,\n",
       "    2265,\n",
       "    4198,\n",
       "    7169,\n",
       "    4159,\n",
       "    3974,\n",
       "    487,\n",
       "    7170,\n",
       "    7589,\n",
       "    2263,\n",
       "    4890,\n",
       "    7171,\n",
       "    4146,\n",
       "    4112,\n",
       "    7158,\n",
       "    7561,\n",
       "    6703,\n",
       "    1618,\n",
       "    2339,\n",
       "    9165,\n",
       "    7167,\n",
       "    5707,\n",
       "    6661,\n",
       "    4427,\n",
       "    8625],\n",
       "   'current_state_bm25_context_scores': [35.000604953158465,\n",
       "    34.57220562792213,\n",
       "    33.9433918217069,\n",
       "    28.478357275248385,\n",
       "    26.862605410003127,\n",
       "    26.611482133388318,\n",
       "    26.371746520230623,\n",
       "    25.963433361372232,\n",
       "    25.503990443701735,\n",
       "    24.441474382550123,\n",
       "    24.179307409922153,\n",
       "    24.07609332676864,\n",
       "    23.49684567617164,\n",
       "    23.440433803182138,\n",
       "    23.263260140590177,\n",
       "    22.947139571252542,\n",
       "    22.91041772499119,\n",
       "    22.861639772397876,\n",
       "    22.525894406560965,\n",
       "    22.456285758648296,\n",
       "    22.424952765862688,\n",
       "    22.21561267130555,\n",
       "    22.123442738972315,\n",
       "    22.10210021242674,\n",
       "    21.981024376070167,\n",
       "    21.673431402320347,\n",
       "    21.406701918908663,\n",
       "    21.3445859935901,\n",
       "    21.281711326465757,\n",
       "    20.958755243002443,\n",
       "    20.881876906399807,\n",
       "    20.7908962628048,\n",
       "    20.705307208472373,\n",
       "    20.6392763172628,\n",
       "    20.493424647578,\n",
       "    20.373397595608836,\n",
       "    20.26166234013928,\n",
       "    20.2155847533004,\n",
       "    20.17474697933905,\n",
       "    20.15752492596419,\n",
       "    20.036328940149254,\n",
       "    20.034928169664596,\n",
       "    19.96842009975062,\n",
       "    19.92490653158365,\n",
       "    19.835861649834516,\n",
       "    19.79309725944666,\n",
       "    19.712017953083187,\n",
       "    19.59718983900988,\n",
       "    19.583896845159554,\n",
       "    19.56961489911309],\n",
       "   'current_state_hybrid_fused_scores': [0.030798389007344232,\n",
       "    0.0304147465437788,\n",
       "    0.029726775956284153,\n",
       "    0.0277569392348087,\n",
       "    0.027252906976744186],\n",
       "   'top_topic_chunk_ids': [7153, 7159, 7161, 7160, 7152],\n",
       "   'top_topic_chunk_similarity': [0.30644143099395205,\n",
       "    0.2841925858289274,\n",
       "    0.26118468157750296,\n",
       "    0.2579025923956211,\n",
       "    0.24843865272397003],\n",
       "   'weight_coherence': 0.3333333333333333,\n",
       "   'weight_query_relevance': 0.3333333333333333,\n",
       "   'weight_chunk_count': 0.3333333333333333,\n",
       "   'experiment_tag': 'user_query_history_logging'}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b063813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#### First Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : False,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88110b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Chunk ID: 6661, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 354, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2261, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 657, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4087, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 637, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 2264, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 672, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "#### Chunk ID: 4109, Title: Llama 2: Open Foundation and Fine-Tuned Chat Models, Publish Date: 2023-07 , Link: https://arxiv.org/abs/2307.09288, # of Tokens: 599, Section: **Llama 2: Open Foundation and Fine-Tuned Chat Models** ---> **A Appendix**\n",
      "Final User State Length: 4\n",
      "\n",
      "Response: Hello! I am doing well, thank you for asking! I am ready to provide you with detailed and comprehensive information based on your requests, drawing upon insights from AI/ML research papers.\n",
      "\n",
      "\n",
      "#### Chunk ID: 4005, Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement\n",
      "  Learning Algorithm, Publish Date: 2017-02 , Link: https://arxiv.org/abs/1712.01815, # of Tokens: 720, Section: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm ---> **Methods** ---> **Example games** ---> 17\n",
      "#### Chunk ID: 4525, Title: Mastering Chess and Shogi by Self-Play with a General Reinforcement\n",
      "  Learning Algorithm, Publish Date: 2017-02 , Link: https://arxiv.org/abs/1712.01815, # of Tokens: 1389, Section: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm ---> **Methods** ---> **Example games** ---> 17\n",
      "Final User State Length: 5\n",
      "\n",
      "Response: Let's revisit the question \"Is the sky blue?\"\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "The question \"Is the sky blue?\" is a fundamental example of a query that tests a machine's ability to recall factual knowledge. As noted in prior responses, this falls under the umbrella of question-answering (QA) within NLP. An AI model, particularly a large language model (LLM), leverages its training data to provide a direct and accurate answer. The simplicity of the question belies its importance in understanding how AI models acquire and apply knowledge.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "Answering \"Is the sky blue?\" involves the following key elements, echoing the principles previously established:\n",
      "\n",
      "*   **Knowledge Retrieval:** The model accesses its pre-existing knowledge about the world, learned through training on vast text datasets.\n",
      "*   **Question Understanding:** The model parses the question to understand its intent and context.\n",
      "*   **Answer Generation:** The model formulates a response based on its internal understanding and knowledge.\n",
      "\n",
      "In essence, the model retrieves the fact that the sky is blue from its internal representation of the world. Context Chunk 1's examples directly demonstrate this capability in action.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "The context chunks provide clear examples to illustrate this point. Context Chunk 1, for instance, contains a conversation where the human asks \"is sky blue or black?\" and the machine answers \"blue.\" This simple interaction showcases the model's ability to respond correctly to a direct question. Further examples from this chunk also demonstrate the models' ability to answer general knowledge questions like \"What is the color of the sky?\" with \"blue.\"\n",
      "\n",
      "Context Chunk 0 extends this by showing the same question in multiple languages. This demonstrates how the LLMs can perform cross-linguistic mapping, retrieving the same core knowledge regardless of the question's language.\n",
      "\n",
      "Context Chunk 2 does not directly provide an answer to the question, however, it does show a series of questions and their corresponding answers.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "To build upon this, here are some related areas for exploration:\n",
      "\n",
      "*   **Knowledge Representation and Reasoning:** Explore how AI models store and utilize knowledge. Consider different methods for organizing information, such as knowledge graphs, and investigate how these methods can improve question-answering capabilities.\n",
      "*   **Multilingual Question Answering:** Investigate how to create QA systems that can understand and answer questions in multiple languages. This includes understanding the challenges and techniques associated with cross-lingual knowledge retrieval and transfer.\n",
      "*   **Fact Verification and Counterfactual Reasoning**: Investigate the model's ability to distinguish between true and false statements, and the capability to hypothesize alternate scenarios based on available information.\n",
      "\n",
      "I hope this response furthers your understanding.\n",
      "\n",
      "\n",
      "#### Chunk ID: 6395, Title: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition, Publish Date: 2018-04 , Link: https://arxiv.org/abs/1804.03209, # of Tokens: 385, Section: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition ---> **5 Collection** ---> **5.3 Implementation**\n",
      "#### Chunk ID: 2949, Title: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition, Publish Date: 2018-04 , Link: https://arxiv.org/abs/1804.03209, # of Tokens: 543, Section: Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition ---> **5 Collection** ---> **5.3 Implementation**\n",
      "Final User State Length: 6\n",
      "\n",
      "Response: Let's delve into the question, \"What does the ocean sound like?\"\n",
      "\n",
      "### 1. Introductory Overview\n",
      "\n",
      "The query \"What does the ocean sound like?\" is a fascinating example of a task for an AI model. It goes beyond simple factual recall, demanding the model to generate a description of a sensory experience. The model must leverage its knowledge of the ocean and its associated sounds, and then translate this into a descriptive, easily understandable narrative.\n",
      "\n",
      "### 2. Technical Overview\n",
      "\n",
      "To address this question, an AI model would need to combine several capabilities:\n",
      "\n",
      "*   **Knowledge Retrieval:** The model must access its learned information about the ocean. This knowledge is typically embedded within its parameters, acquired through training on vast text and, potentially, audio datasets.\n",
      "*   **Auditory Sensory Understanding:** The model needs to associate the concept of \"ocean\" with characteristic sounds.\n",
      "*   **Descriptive Language Generation:** It has to generate text that describes auditory experiences, using terms such as \"waves crashing,\" \"seabirds calling,\" or \"lapping water.\"\n",
      "*   **Natural Language Generation (NLG):** The model must formulate a coherent response that conveys the information in a natural and engaging manner.\n",
      "\n",
      "Given the provided context chunks, an AI model can utilize its question-answering abilities to address the current query. However, the model must also make inferences about the characteristics of the ocean based on its training data.\n",
      "\n",
      "### 3. Example-Based Expansion\n",
      "\n",
      "Several context chunks showcase relevant concepts. Context Chunk 1 presents the model's capability for performing question-answering within a simple Q&A structure. This capability shows how the model can associate concepts with corresponding attributes. The model associates concepts like \"sky\" with the color \"blue\". In the current scenario, the model must connect the concept of \"ocean\" to associated sounds.\n",
      "\n",
      "Context Chunk 4 introduces the concept of background noise. This is not directly related to the sound of the ocean, but it does shed light on challenges in audio processing and the need to distinguish between desired and undesired audio signals.\n",
      "\n",
      "### 4. Broader Exploration\n",
      "\n",
      "To further expand your understanding, consider the following related topics:\n",
      "\n",
      "*   **Audio Generation and Processing:** Explore how AI models are employed in generating and processing audio, including techniques for creating realistic soundscapes, sound identification, and audio enhancement.\n",
      "*   **Multimodal AI:** Investigate AI models that integrate multiple modalities (text, audio, visual information, etc.) to create more comprehensive and human-like understandings of the world. This is especially relevant when an AI is expected to offer details based on various sensory inputs.\n",
      "*   **Creative Writing with AI:** Examine the use of AI in creative writing, encompassing poetry, storytelling, and the generation of descriptive texts. This can offer insights into an AI's capacity to create narratives and descriptions based on prompts.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "#### First Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_1,\n",
    ")\n",
    "\n",
    "#######################################\n",
    "#### Second Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_2 = {\n",
    "    'USER_INPUT_QUERY' : 'is the sky blue?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_2,\n",
    ")\n",
    "\n",
    "#######################################\n",
    "#### Final Query\n",
    "#######################################\n",
    "\n",
    "my_front_end_settings_query_3 = {\n",
    "    'USER_INPUT_QUERY' : 'what does the ocean sound like?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# When the user decides to end the session (e.g., after the last query)\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=my_front_end_settings_query_3, \n",
    ")\n",
    "\n",
    "# End the current user's session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59efb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44266c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query_number', 'query_text', 'query_embedding', 'rag_used', 'history_used', 'rag_and_history_used', 'bm25_used', 'topic_retrieval_used', 'desired_lookback_window_size', 'actual_lookback_window_size', 'desired_top_k_chunks', 'query_start_time', 'query_finish_time', 'query_processing_time', 'context_ids_utilized', 'context_ids_source', 'response_text', 'prompt_token_count', 'candidates_token_count', 'total_token_count', 'system_prompt_used', 'dynamic_prompt_body', 'current_state_context_ids', 'current_state_context_scores', 'retrieval_top_k', 'retrieval_method', 'avg_similarity_to_context', 'max_similarity_to_context', 'top_context_id', 'top_context_score', 'considered_prior_state_ids', 'utilized_prior_state_ids', 'filter_similarity_score_threshold', 'filter_similarity_score_mask', 'bm25_RRF_constant', 'bm25_multiplier', 'current_state_bm25_context_ids', 'current_state_bm25_context_scores', 'current_state_hybrid_fused_scores', 'top_topic_chunk_ids', 'top_topic_chunk_similarity', 'weight_coherence', 'weight_query_relevance', 'weight_chunk_count', 'experiment_tag'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2632dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_chat_query_num = list(user_chat_settings['user_query_state_history'].keys())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2f999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let\\'s delve into the question, \"What does the ocean sound like?\"\\n\\n### 1. Introductory Overview\\n\\nThe query \"What does the ocean sound like?\" is a fascinating example of a task for an AI model. It goes beyond simple factual recall, demanding the model to generate a description of a sensory experience. The model must leverage its knowledge of the ocean and its associated sounds, and then translate this into a descriptive, easily understandable narrative.\\n\\n### 2. Technical Overview\\n\\nTo address this question, an AI model would need to combine several capabilities:\\n\\n*   **Knowledge Retrieval:** The model must access its learned information about the ocean. This knowledge is typically embedded within its parameters, acquired through training on vast text and, potentially, audio datasets.\\n*   **Auditory Sensory Understanding:** The model needs to associate the concept of \"ocean\" with characteristic sounds.\\n*   **Descriptive Language Generation:** It has to generate text that describes auditory experiences, using terms such as \"waves crashing,\" \"seabirds calling,\" or \"lapping water.\"\\n*   **Natural Language Generation (NLG):** The model must formulate a coherent response that conveys the information in a natural and engaging manner.\\n\\nGiven the provided context chunks, an AI model can utilize its question-answering abilities to address the current query. However, the model must also make inferences about the characteristics of the ocean based on its training data.\\n\\n### 3. Example-Based Expansion\\n\\nSeveral context chunks showcase relevant concepts. Context Chunk 1 presents the model\\'s capability for performing question-answering within a simple Q&A structure. This capability shows how the model can associate concepts with corresponding attributes. The model associates concepts like \"sky\" with the color \"blue\". In the current scenario, the model must connect the concept of \"ocean\" to associated sounds.\\n\\nContext Chunk 4 introduces the concept of background noise. This is not directly related to the sound of the ocean, but it does shed light on challenges in audio processing and the need to distinguish between desired and undesired audio signals.\\n\\n### 4. Broader Exploration\\n\\nTo further expand your understanding, consider the following related topics:\\n\\n*   **Audio Generation and Processing:** Explore how AI models are employed in generating and processing audio, including techniques for creating realistic soundscapes, sound identification, and audio enhancement.\\n*   **Multimodal AI:** Investigate AI models that integrate multiple modalities (text, audio, visual information, etc.) to create more comprehensive and human-like understandings of the world. This is especially relevant when an AI is expected to offer details based on various sensory inputs.\\n*   **Creative Writing with AI:** Examine the use of AI in creative writing, encompassing poetry, storytelling, and the generation of descriptive texts. This can offer insights into an AI\\'s capacity to create narratives and descriptions based on prompts.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_chat_settings['user_query_state_history'][most_recent_chat_query_num]['response_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45241a04",
   "metadata": {},
   "source": [
    "# IMPORTANT - A Potential Guide to Handling Multiple Users / Scheduling\n",
    "\n",
    "If you want to swap to process different user queries; you will have to re-route before each input query. This means that if you want to serve multiple users, you have to save to set `**end_session=True**` for every job from every user, otherwise the qdrant store that handles searches against user queries will be populated by everybody's history at the same time. Furthermore, it will over-write itself because the query_ids that it utilizes (values such as `1`, `2`, `3`) will be overwritten by multiple users using the system at the same time.\n",
    "\n",
    "Handling this by creating a seperate collection for each user is not the worst solution at our scale, but making this dynamic would be a massive pain.\n",
    "\n",
    "## Scheduling Guide\n",
    "\n",
    "In order to handle multiple users; you process the first input query, and then hold their routing information / information until the next input query. If the next input query's source is different, we can load the new routing information and process their query. If the next input query's source is the same, we can simply process the query without having to re-load their information.\n",
    "\n",
    "### Variable Definitions:\n",
    "\n",
    "Each user has query routing information based on their UUID / their connection information:\n",
    "\n",
    "```python\n",
    "##### Structure of folders:\n",
    "# user_output/{task_folder}/{user_output_folder}/{saved_chats_topic_name}\n",
    "\n",
    "user_chat_group: str = 'saved_chats' # this stores all user chats\n",
    "chat_user: str = 'test_user2' # this stores a specific user's multiple chat\n",
    "user_chat_topic: str = 'my_chat' # this is a specific user's single chat\n",
    "\n",
    "# path would be = 'user_output/saved_chats/test_user2/my_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Each Job has information structured:\n",
    "\n",
    "```python\n",
    "job = {\n",
    "    'USER_INPUT_QUERY' : str,\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : int,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : int,\n",
    "    'RAG_SWITCH' : bool,\n",
    "    'HISTORY_SWITCH' : bool,\n",
    "    'BM25_SWITCH' : bool,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : bool,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : float,\n",
    "}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "Say we have 2 users:\n",
    "\n",
    "our_unique_users = [`user_1`, `user_2`]\n",
    "\n",
    "<br>\n",
    "\n",
    "And they input jobs that can be sequentially ordered:\n",
    "\n",
    "`user_1`: `job_1`, `job_2`, `job_4`\n",
    "\n",
    "`user_2`: `job_3`, `job_5`\n",
    "\n",
    "<br>\n",
    "\n",
    "Where a schedule is represented like this:\n",
    "\n",
    "all_jobs_t0 =        [ `job_1`,  `job_2`,  `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t0 = [`user_1`, `user_1`, `user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Where `t0` refers to the time step.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Scheudling Example Code:\n",
    "\n",
    "#### Time step t=0:\n",
    "\n",
    "We start with `user_1` who has `job_1`. We have to load their routing information and process their job so that we can serve them with their response.\n",
    "\n",
    "``` python\n",
    "#######################################\n",
    "#### First Query: `job_1` from `user_1`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_1` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_1'\n",
    "user_chat_topic = 'user_1_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_1 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_1,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=1:\n",
    "\n",
    "Updated queue at time step 1:\n",
    "\n",
    "all_jobs_t1 =        [ `job_2`,  `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t1 = [`user_1`, `user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Nobody else inputs a query before `user_1` inputs `job_2`, so their data does not have to be permanently saved and reloaded in order to process `job_2`. \n",
    "\n",
    "```python\n",
    "#######################################\n",
    "#### Second Query `job_2` from `user_1`\n",
    "#######################################\n",
    "\n",
    "job_2 = {\n",
    "    'USER_INPUT_QUERY' : 'is the sky blue?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=job_2,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=2:\n",
    "\n",
    "Updated queue at time step 2:\n",
    "\n",
    "all_jobs_t2 =        [ `job_3`,  `job_4`, `job_5`]\n",
    "all_jobs_source_t2 = [`user_2`, `user_1`,`user_2`]\n",
    "\n",
    "Now, `user_2` is the owner of the next job, `job_3`. However, we are still holding `user_1` routing information; we have to end their session and permanently save their session information. After this, we can get `user_2` routing information in to process `job_3`.\n",
    "\n",
    "```python\n",
    "# End `user_1` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Third Query `job_3` from `user_2`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_2` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_2'\n",
    "user_chat_topic = 'user_2_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_3 = {\n",
    "    'USER_INPUT_QUERY' : 'what does the ocean sound like?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# When the user decides to end the session (e.g., after the last query)\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=user_chat_settings, \n",
    "    frontend_inputs=job_3, \n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=3:\n",
    "\n",
    "Updated queue at time step 3:\n",
    "\n",
    "all_jobs_t3 =        [ `job_4`, `job_5`]\n",
    "all_jobs_source_t3 = [`user_1`,`user_2`]\n",
    "\n",
    "Now, `user_1` has `job_4`. We need to end `user_2` session and save their information to process `job_4`.\n",
    "\n",
    "```python\n",
    "# End `user_2` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Fourth Query `job_4` from `user_1`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_1` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_1'\n",
    "user_chat_topic = 'user_1_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_4 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_4,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Time step t=4:\n",
    "\n",
    "Updated queue at time step 4:\n",
    "\n",
    "all_jobs_t4 =        [ `job_5`]\n",
    "all_jobs_source_t4 = [`user_2`]\n",
    "\n",
    "Finally, `user_2` inputs `job_5`. We permanently save `user_1` information and route to `user_2`.\n",
    "\n",
    "```python\n",
    "# End `user_1` session and save their history permanently\n",
    "end_user_session(user_chat_settings=user_chat_settings)\n",
    "\n",
    "#######################################\n",
    "#### Fifth Query `job_5` from `user_2`\n",
    "#######################################\n",
    "\n",
    "# Now we initialize `user_2` chat using their routing information\n",
    "user_chat_group = 'saved_chats'\n",
    "chat_user = 'user_2'\n",
    "user_chat_topic = 'user_2_chat'\n",
    "\n",
    "initial_user_chat_settings = initialize_user_chat(\n",
    "    task_folder=user_chat_group,\n",
    "    user_output_folder=chat_user,\n",
    "    saved_chats_topic_name=user_chat_topic, \n",
    ")\n",
    "\n",
    "job_5 = {\n",
    "    'USER_INPUT_QUERY' : 'hello, how are you today?',\n",
    "    'DESIRED_HISTORY_WINDOW_SIZE' : 3,\n",
    "    'DESIRED_CONTEXT_CHUNKS_TOP_K' : 5,\n",
    "    'RAG_SWITCH' : True,\n",
    "    'HISTORY_SWITCH' : True,\n",
    "    'BM25_SWITCH' : True,\n",
    "    'TOPIC_RETRIEVAL_SWITCH' : True,\n",
    "    'HISTORIC_QUERY_SIMILARITY_THRESHOLD' : 0.3,\n",
    "}\n",
    "\n",
    "# Call for each query\n",
    "user_chat_settings = input_query(\n",
    "    user_chat_settings=initial_user_chat_settings, \n",
    "    frontend_inputs=job_5,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "Lastly, we hold of on saving permanent history until the next job enters our queue. If the user who we just served (in this case, `user_2`) decides to close out of the application or start a new chat, trigger a permanent save of their last chat and keep waiting for the next job. \n",
    "\n",
    "And now we have completed all the scheduled jobs, while maintaining a permanent session history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833716bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
